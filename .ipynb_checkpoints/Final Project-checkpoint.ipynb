{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom Entailment Model\n",
    "\n",
    "**Remark**: This is available because the original experiment used a custom entailment model. If you do not want to run this code, you do not have to.\n",
    "\n",
    "**Note**: If you choose to run this code, please follow the steps in `./models/textual_entailment.ipynb` to download the `textual_entailment.tar.gz` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from models.textual_entailment import TextualEntailment\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "DEVICE = torch.cuda.current_device()\n",
    "\n",
    "class Entailment:\n",
    "    \n",
    "    def __init__(self, load_elmo: bool = True, input_dim: int = 400):\n",
    "        self.input_dim = 400\n",
    "        if load_elmo:\n",
    "            self._frozen = Predictor.from_path(\"./models/textual_entailment.tar.gz\", cuda_device=DEVICE)\n",
    "        self._model = self._model()\n",
    "        \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            0: 'SUPPORTS',\n",
    "            1: 'NOT ENOUGH INFO',\n",
    "            2: 'REFUTES'\n",
    "        }\n",
    "        return switcher[label]\n",
    "\n",
    "    def _model(self) -> nn.Sequential:\n",
    "        \"\"\" Builds the MLP for the classification \"\"\"\n",
    "        model = nn.Sequential(\n",
    "          nn.Linear(400,100),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(100,3),\n",
    "          nn.Softmax(dim=1)\n",
    "        )\n",
    "        model.cuda()\n",
    "        return model\n",
    "    \n",
    "    def save(self, path: str = './models/entailment.model') -> None:\n",
    "        \"\"\" Saves the model to the file \"\"\"\n",
    "        torch.save(self._model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path: str = './models/entailment.model') -> bool:\n",
    "        \"\"\" Loads the model, if possible \"\"\"\n",
    "        if os.path.exists(path):\n",
    "            self._model.load_state_dict(torch.load(path))\n",
    "            return True\n",
    "        raise ValueError(f\"Path {path} is not valid\")\n",
    "\n",
    "    def entail_dataset(self, dataset: Dataset, batch_size: int = 50, \n",
    "                       total_size: int = 30000) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        dataset = dataset.shuffle(seed=42)[:total_size]\n",
    "        X = self.entail_batch(dataset['premise'], dataset['hypothesis'], batch_size)\n",
    "        y = torch.tensor(dataset['label'])\n",
    "        return X, y\n",
    "    \n",
    "    def entail_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            \n",
    "        iters = int(total_size / batch_size)\n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            batch_json = self._frozen.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['aggregate_input'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "        \n",
    "    def predict(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> torch.Tensor:\n",
    "        \"\"\" Predicts the next word for the text \"\"\"\n",
    "        embedding = self.entail_batch(premises, hypotheses)\n",
    "        embedding = embedding.cuda()\n",
    "        preds = self._model(embedding)\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\" Evaluates the model on the test set \"\"\"\n",
    "        n, _ = X.size()\n",
    "        val = self._model(X)\n",
    "        preds = torch.argmax(val, dim=1)\n",
    "        total = float(sum(preds == y)) / n\n",
    "        return total\n",
    "\n",
    "    def fit(self, X_train: torch.Tensor, y_train: torch.Tensor, X_val: torch.Tensor, y_val: torch.Tensor, \n",
    "            batch_size: int = 32, epochs: int = 100, shuffle: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\" Fits the data on the model \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self._model.parameters(), lr=1E-3)\n",
    "        \n",
    "        X_train = X_train.cuda()\n",
    "        y_train = y_train.long().cuda()\n",
    "        X_val = X_val.cuda()\n",
    "        y_val = y_val.long().cuda()\n",
    "        \n",
    "        n, _ = X_train.shape\n",
    "        tr_iters = int(n / batch_size)\n",
    "        \n",
    "        n, _ = X_val.shape\n",
    "        val_iters = int(n / batch_size)\n",
    "        \n",
    "        train_loss = np.zeros(epochs)\n",
    "        train_acc = np.zeros(epochs)\n",
    "        validation_loss = np.zeros(epochs)\n",
    "        validation_acc = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            # Train and Validation Loss\n",
    "            total_loss = self._fit(X_train, y_train, criterion, optimizer, tr_iters, batch_size, shuffle)\n",
    "            val_loss = self._loss(X_val, y_val, criterion, val_iters, batch_size, shuffle)\n",
    "            \n",
    "            total_acc = self.evaluate(X_train, y_train)\n",
    "            val_acc = self.evaluate(X_val, y_val)\n",
    "            \n",
    "            print('[%d] loss: %.7f acc: %.7f \\t val_loss: %.7f val_acc: %.7f' % \n",
    "                  (epoch + 1, total_loss, total_acc, val_loss, val_acc)\n",
    "                 )\n",
    "            \n",
    "            train_loss[epoch] = total_loss\n",
    "            train_acc[epoch] = total_acc\n",
    "            \n",
    "            validation_loss[epoch] = val_loss\n",
    "            validation_acc[epoch] = val_acc\n",
    "        return train_loss, train_acc, validation_loss, validation_acc\n",
    "    \n",
    "    def _fit(self, X: torch.Tensor, y: torch.Tensor, criterion: nn.CrossEntropyLoss, \n",
    "             optimizer: optim.Adam, num_iters: int, batch_size: int = 32, \n",
    "             shuffle: bool = True) -> Tuple[float, float]:\n",
    "        \"\"\" Runs an epoch of training \"\"\"\n",
    "        total_loss = 0.0\n",
    "        if shuffle: \n",
    "            indices = np.random.permutation(range(X.shape[0]))\n",
    "        else:\n",
    "            indices = list(range(X.shape[0]))\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_iters):\n",
    "            end = start + batch_size\n",
    "            i = indices[start:end]\n",
    "            xi, yi = X[i], y[i]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self._model(xi)\n",
    "            loss = criterion(outputs, yi)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            start = end\n",
    "\n",
    "        total_loss /= X.shape[0]\n",
    "        return total_loss\n",
    "    \n",
    "    def _loss(self, X: torch.Tensor, y: torch.Tensor, criterion: nn.CrossEntropyLoss, \n",
    "              num_iters: int, batch_size: int = 32, shuffle: bool = True) -> Tuple[float, float]:\n",
    "        \"\"\" Runs an epoch of training \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        if shuffle: \n",
    "            indices = np.random.permutation(range(X.shape[0]))\n",
    "        else:\n",
    "            indices = list(range(X.shape[0]))\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_iters):\n",
    "            end = start + batch_size\n",
    "            i = indices[start:end]\n",
    "            xi, yi = X[i], y[i]\n",
    "\n",
    "            # forward + loss\n",
    "            outputs = self._model(xi)\n",
    "            loss = criterion(outputs, yi)\n",
    "            total_loss += loss.item()\n",
    "            start = end\n",
    "        total_loss /= X.shape[0]\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: The below code regenerates the entailments. **This may take a long time if you do not have a GPU.** I recommend only running this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/jmack/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "snli_data = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550152, 3), (10000, 3), (10000, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_data['train'].shape, snli_data['validation'].shape, snli_data['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmack/anaconda3/envs/final-project/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "entailment = Entailment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a007e1013313495fa99b8e82968a7b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61c8fd06b2f4edf98483e6cdff6304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed93ed5c53b3477d91e107b995554186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:15<00:00,  1.96it/s]\n",
      "100%|██████████| 500/500 [04:24<00:00,  1.89it/s]\n",
      "100%|██████████| 500/500 [04:18<00:00,  1.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([150000, 400]), torch.Size([150000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_balanced_data(entailment: Entailment, data: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Gets the training data for the entailment model \"\"\"\n",
    "    entailments = data.filter(lambda x: x['label'] == 0)\n",
    "    neutrals = data.filter(lambda x: x['label'] == 1)\n",
    "    contradictions = data.filter(lambda x: x['label'] == 2)\n",
    "    X_entail, y_entail = entailment.entail_dataset(entailments, batch_size=100, total_size=50000)\n",
    "    X_neutral, y_neutral = entailment.entail_dataset(neutrals, batch_size=100, total_size=50000)\n",
    "    X_contra, y_contra = entailment.entail_dataset(contradictions, batch_size=100, total_size=50000)\n",
    "    X_train = torch.vstack((X_entail, X_neutral))\n",
    "    X_train = torch.vstack((X_train, X_contra))\n",
    "    y_train = torch.hstack((y_entail, y_neutral))\n",
    "    y_train = torch.hstack((y_train, y_contra))\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train, y_train = get_balanced_data(entailment, snli_data['train'])\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(50000), tensor(50000), tensor(50000))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 0), sum(y_train == 1), sum(y_train == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = './data/x_train.pt'\n",
    "torch.save(X_train, x_train_path)\n",
    "\n",
    "y_train_path = './data/y_train.pt'\n",
    "torch.save(y_train, y_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:51<00:00,  1.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 400]), torch.Size([10000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_val = snli_data['validation'].shape[0]\n",
    "X_val, y_val = entailment.entail_dataset(snli_data['validation'], batch_size=100, total_size=total_val)\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_path = './data/x_validation.pt'\n",
    "torch.save(X_val, x_val_path)\n",
    "\n",
    "y_val_path = './data/y_validation.pt'\n",
    "torch.save(y_val, y_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 400]), torch.Size([10000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_test = snli_data['test'].shape[0]\n",
    "X_test, y_test = entailment.entail_dataset(snli_data['test'], batch_size=100, total_size=total_test)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_path = './data/x_test.pt'\n",
    "torch.save(X_test, x_test_path)\n",
    "\n",
    "y_test_path = './data/y_test.pt'\n",
    "torch.save(y_test, y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrays(x_path: str, y_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = torch.load(x_path)\n",
    "    y = torch.load(y_path)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150000, 400]), torch.Size([150000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_path = './data/x_train.pt'\n",
    "y_train_path = './data/y_train.pt'\n",
    "\n",
    "X_train, y_train = load_arrays(x_train_path, y_train_path)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(50000), tensor(50000), tensor(50000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 0), sum(y_train == 1), sum(y_train == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9842, 400]), torch.Size([9842]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_path = './data/x_validation.pt'\n",
    "y_val_path = './data/y_validation.pt'\n",
    "\n",
    "X_val, y_val = load_arrays(x_val_path, y_val_path)\n",
    "\n",
    "pos = torch.nonzero(y_val != -1)  # Remove -1\n",
    "pos = pos.reshape(pos.size()[0])  # Remove -1\n",
    "\n",
    "X_val = X_val[pos]\n",
    "y_val = y_val[pos]\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3329), tensor(3235))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_val == 0), sum(y_val == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9824, 400]), torch.Size([9824]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_path = './data/x_test.pt'\n",
    "y_test_path = './data/y_test.pt'\n",
    "\n",
    "X_test, y_test = load_arrays(x_test_path, y_test_path)\n",
    "\n",
    "pos = torch.nonzero(y_test != -1)  # Remove -1\n",
    "pos = pos.reshape(pos.size()[0])  # Remove -1\n",
    "\n",
    "X_test = X_test[pos]\n",
    "y_test = y_test[pos]\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3368), tensor(3219))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test == 0), sum(y_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment = Entailment(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.0207126 acc: 0.8952933 \t val_loss: 0.0214792 val_acc: 0.8592766\n",
      "[2] loss: 0.0203855 acc: 0.9008400 \t val_loss: 0.0213397 val_acc: 0.8647633\n",
      "[3] loss: 0.0203360 acc: 0.9019200 \t val_loss: 0.0213814 val_acc: 0.8631376\n",
      "[4] loss: 0.0202902 acc: 0.9003333 \t val_loss: 0.0214351 val_acc: 0.8605974\n",
      "[5] loss: 0.0202420 acc: 0.9039200 \t val_loss: 0.0213079 val_acc: 0.8658809\n",
      "[6] loss: 0.0202264 acc: 0.9060467 \t val_loss: 0.0212936 val_acc: 0.8671002\n",
      "[7] loss: 0.0201762 acc: 0.9059733 \t val_loss: 0.0212571 val_acc: 0.8677098\n",
      "[8] loss: 0.0201547 acc: 0.9081133 \t val_loss: 0.0212586 val_acc: 0.8669986\n",
      "[9] loss: 0.0201272 acc: 0.9068333 \t val_loss: 0.0213590 val_acc: 0.8625279\n",
      "[10] loss: 0.0201058 acc: 0.9064133 \t val_loss: 0.0213057 val_acc: 0.8649665\n"
     ]
    }
   ],
   "source": [
    "loss, acc, val_loss, val_acc = entailment.fit(X_train, y_train, X_val, y_val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = list(range(10))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545399022801303"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment.evaluate(X_test.cuda(), y_test.long().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545399022801303"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Entailment(False)\n",
    "model.load()\n",
    "model.evaluate(X_test.cuda(), y_test.long().cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the FEVER Dataset & Entailment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fever (/home/jmack/.cache/huggingface/datasets/fever/v1.0/1.0.0/fe391c4f48669454ae0d368997430e6fa476aacb476d930d3328b67356e74625)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18567, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('fever', 'v1.0')\n",
    "data = dataset['paper_test']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: You can use the above model or this model, the results are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmack/anaconda3/envs/final-project/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.cuda.current_device()\n",
    "url = \"https://storage.googleapis.com/allennlp-public-models/decomposable-attention-elmo-2020.04.09.tar.gz\"\n",
    "entailment_model = Predictor.from_path(url, cuda_device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Facebook Experiment\n",
    "\n",
    "1. Implement automatic masking\n",
    "2. Get the top 1 prediction from the LM \n",
    "3. Fill in the mask\n",
    "4. Use the claim and filled in sentence and input into an entailment model\n",
    "5. Input entailment into MLP for final fact-verification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Iterable, List, Tuple\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, tokenizer, unmasker, model):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unmasker = unmasker.to(self.device)\n",
    "        self.model = model\n",
    "        self.vocab = tokenizer.get_vocab()\n",
    "        self.stop_words = dict(zip(self.nlp.Defaults.stop_words, range(len(self.nlp.Defaults.stop_words))))\n",
    "        \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, save: bool = False, load: bool = False,\n",
    "                 path: str = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            dataset = self.load(path)\n",
    "            return self.evaluate_dataset(dataset)\n",
    "        \n",
    "        print(\"Masking the claims and filling the masks for the dataset\")\n",
    "        data = self.mask_and_fill(dataset, limit)\n",
    "        if save:\n",
    "            print(\"Saving checkpoint...\")\n",
    "            data.to_csv(path)\n",
    "        return self.evaluate_dataset(data)\n",
    "    \n",
    "    def load(self, path: str) -> Dataset:\n",
    "        \"\"\" Loads the dataset from a file \"\"\"\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            return load_dataset('csv', data_files=[path])['train']\n",
    "        raise ValueError(f\"Path {path} does not exist or is not a file\")\n",
    "        \n",
    "    def mask_and_fill(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._mask)\n",
    "        data = data.filter(lambda x: x['masked'] is not None)\n",
    "        data = data.map(self._fill)\n",
    "        return data\n",
    "    \n",
    "    def _mask(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the last named entity in the string \"\"\"\n",
    "        x['target'] = None\n",
    "        x['masked'] = None\n",
    "        x['label'] = self._map(x['label'])\n",
    "        claim = x['claim']\n",
    "        doc = self.nlp(claim, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        ents = doc.ents\n",
    "        if ents:\n",
    "            target = self._get_target(ents)\n",
    "            if target:  # If the target is not in the vocab, skip the entry\n",
    "                masked = self.mask_token.join(claim.rsplit(target, 1))\n",
    "                x['target'] = target\n",
    "                x['masked'] = masked\n",
    "        return x\n",
    "    \n",
    "    def _get_target(self, ents) -> Optional[str]:\n",
    "        \"\"\" Gets the ideal target \"\"\"\n",
    "        for i in reversed(range(len(ents))):\n",
    "            words = ents[i].text.split()\n",
    "            for word in words:\n",
    "                if not self.stop_words.get(word.lower()) and self.vocab.get(word):\n",
    "                    return word\n",
    "        return None\n",
    "            \n",
    "    def _fill(self, x: dict) -> dict:\n",
    "        \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "        x['pred'] = self._predict(x['masked'])\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, x['pred'])\n",
    "        return x\n",
    "    \n",
    "    def _predict(self, masked_claim: str) -> str:\n",
    "        \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "        tokens = self.tokenizer(masked_claim, return_tensors='pt')\n",
    "        masked_index = torch.nonzero(tokens['input_ids'][0] == self.tokenizer.mask_token_id, as_tuple=False)\n",
    "        # Fill mask pipeline supports only one ${mask_token} per sample\n",
    "        num = np.prod(masked_index.shape)\n",
    "        if num > 1 or num < 1:\n",
    "            print(masked_claim, tokens, masked_index)\n",
    "            raise ValueError(f\"Pipeline only supports one masked index: {num} is not supported\")\n",
    "\n",
    "        outputs = self.unmasker(**tokens.to(self.device))\n",
    "        logits = outputs.logits[0, masked_index.item(), :]\n",
    "        probs = logits.softmax(dim=0)\n",
    "        values, predictions = probs.topk(1)\n",
    "        word = self.tokenizer.decode(predictions)\n",
    "        return word.strip()\n",
    "    \n",
    "    def predict_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            iters = 1\n",
    "        else:\n",
    "            iters = int(total_size / batch_size) + 1\n",
    "            \n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            if end >= total_size:\n",
    "                end = total_size\n",
    "            batch_json = self.model.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['label_probs'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['claim'], dataset['hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def evaluate(self, y_true: List[int], y_pred: List[int]) -> tuple:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        metrics = precision_recall_fscore_support(y_true, y_pred)\n",
    "        macro = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        return metrics, macro, acc\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased-whole-word-masking')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-large-cased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bert_tokenizer, bert_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Only needs to be run once, otherwise you can load the data directly using the second command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bert_y_true, bert_y_pred = pipe(data, save=True, path='./data/bert_paper_test.csv') # Process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-af46031b048626cb\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-af46031b048626cb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 281/281 [01:10<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_y_true, bert_y_pred = pipe(load=True, path='./data/bert_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.40085306, 0.24508734, 0.43024963]),\n",
       "  array([0.84713024, 0.13331354, 0.05637868]),\n",
       "  array([0.54419759, 0.17269231, 0.09969377]),\n",
       "  array([5436, 3368, 5197])),\n",
       " (0.3587300110680662, 0.345607487341901, 0.27219455676900806, None),\n",
       " 0.3819012927648025)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_metrics, bert_macro, bert_acc = pipe.evaluate(bert_y_true, bert_y_pred)\n",
    "bert_metrics, bert_macro, bert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Check on Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART\n",
    "\n",
    "- Unknown training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "bart_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bart_tokenizer, bart_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Only needs to be run once, otherwise just load the data directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bart_freeze_y_true, bart_freeze_y_pred = pipe(data, save=True, path='./data/bart_paper_test.csv') # Process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-603c3cb2a5daf17b\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-603c3cb2a5daf17b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [00:58<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_freeze_y_true, bart_freeze_y_pred = pipe(load=True, path='./data/bart_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39118618, 0.19572553, 0.4       ]),\n",
       "  array([0.91259058, 0.06205421, 0.03409353]),\n",
       "  array([0.54762875, 0.09423233, 0.06283167]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.32897057058316165, 0.33624610494039125, 0.23489758530096802, None),\n",
       " 0.3764380243923536)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc = pipe.evaluate(bart_freeze_y_true, bart_freeze_y_pred)\n",
    "bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "- Trained on Bookcorpus, Wikipedia and CC News datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdTokenizer, BigBirdForMaskedLM\n",
    "  \n",
    "bigbird_tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-large\")\n",
    "bigbird_model = BigBirdForMaskedLM.from_pretrained(\"google/bigbird-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bigbird_tokenizer, bigbird_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bigbird_y_true, bigbird_y_pred = pipe(data, save=True, path='./data/bigbird_paper_test.csv') # Process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c87a382d2406e7ce\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-c87a382d2406e7ce/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:00<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "bigbird_y_true, bigbird_y_pred = pipe(load=True, path='./data/bigbird_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39205189, 0.21294559, 0.38491049]),\n",
       "  array([0.86231884, 0.08095578, 0.06933886]),\n",
       "  array([0.53903319, 0.11731266, 0.11750927]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.3299693220495025, 0.33753782668461385, 0.2579517091831368, None),\n",
       " 0.3750540610673817)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_metrics, bb_macro, bb_acc = pipe.evaluate(bigbird_y_true, bigbird_y_pred)\n",
    "bb_metrics, bb_macro, bb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT v2\n",
    "\n",
    "- Model was trained on Bookcorpus and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast, AlbertForMaskedLM\n",
    "  \n",
    "albert_tokenizer = AlbertTokenizerFast.from_pretrained(\"albert-large-v2\")\n",
    "albert_model = AlbertForMaskedLM.from_pretrained(\"albert-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(albert_tokenizer, albert_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "albert_y_true, albert_y_pred = pipe(data, save=True, path='./data/albert_paper_test.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e7b121b58b205f1\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-8e7b121b58b205f1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 102/102 [00:28<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "albert_y_true, albert_y_pred = pipe(load=True, path='./data/albert_paper_test.csv')  # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.35714286, 0.20067454, 0.48881789]),\n",
       "  array([0.85379783, 0.0958132 , 0.07289185]),\n",
       "  array([0.50362136, 0.12970027, 0.12686567]),\n",
       "  array([1751, 1242, 2099])),\n",
       " (0.3488784282576609, 0.34083429586128394, 0.2533957672364542, None),\n",
       " 0.34701492537313433)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_metrics, albert_macro, albert_acc = pipe.evaluate(albert_y_true, albert_y_pred)\n",
    "albert_metrics, albert_macro, albert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting a Context Layer into the Pipeline\n",
    "\n",
    "- We will try using the autoregressive language model GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT2 on CC News Dataset\n",
    "\n",
    "1. Open up a terminal, navigate to `/path/to/project/finetuning/` and run `run_clm.sh`\n",
    "    - You will have to modify the script to output to a directory of your choice.\n",
    "    \n",
    "    \n",
    "2. Wait until it finishes\n",
    "3. **Note**: This only trains GPT2 on 150,000 samples of the CC News dataset. Please adjust the script to suit both your GPU settings and your fine-tuning needs\n",
    "4. I plan to upload the model fine-tuned on the complete dataset at a later date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning BART on FEVER\n",
    "\n",
    "1. Open up a terminal, navigate to `/path/to/project/finetuning/` and run `run_mlm.sh`\n",
    "    * You will have to modify the script to output to a directory of your choice.\n",
    "    \n",
    "    \n",
    "2. Wait until it finishes\n",
    "3. **Note**: This only trains BART on 150,000 samples of the FEVER dataset. Please adjust the script to suit both your GPU settings and your fine-tuning needs\n",
    "4. I plan to upload the model fine-tuned on the complete dataset at a later date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Pipeline - Adding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_utils import logger\n",
    "import logging\n",
    "\n",
    "\n",
    "class PipelineWithContext(Pipeline):\n",
    "    \n",
    "    def __init__(self, tokenizer, unmasker, text_generator, model):\n",
    "        super().__init__(tokenizer, unmasker, model)\n",
    "        self.text_generator = text_generator\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, context: bool = False,\n",
    "                 fill: bool = False, save: bool = False, load: bool = False, path: str = None, \n",
    "                 load_path: str = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save or load the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            load_path = load_path if load_path else path\n",
    "            dataset = self.load(load_path)\n",
    "        \n",
    "        if save:\n",
    "            print(f\"Saving processed data to {path}\")\n",
    "        \n",
    "        if dataset and not load:\n",
    "            context = True\n",
    "            fill = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims...\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['target'] != None)\n",
    "        \n",
    "        if context:\n",
    "            print(\"Adding context to the dataset\")\n",
    "            dataset = self.add_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        if fill:\n",
    "            print(\"Filling the context masks\")\n",
    "            dataset = self.fill_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return self.evaluate_dataset(dataset)\n",
    "        \n",
    "    def add_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._contextualize)\n",
    "        return data\n",
    "    \n",
    "    def _contextualize(self, x: dict) -> str:\n",
    "        \"\"\" Generates context for the claim \"\"\"\n",
    "        context = self.text_generator(x['claim'], max_length=50, do_sample=False)[0]['generated_text']\n",
    "        if len(context) > self.max_len:\n",
    "            context = context[:self.max_len]\n",
    "        x['context'] = context.replace('\\n', ' ')  # Remove newlines to avoid error in Entailment model\n",
    "        return x\n",
    "    \n",
    "    def fill_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Fills the context map \"\"\"\n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._fill)\n",
    "        return data\n",
    "    \n",
    "    def _fill(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the claim and fills the mask \"\"\"\n",
    "        x['masked_context'] = x['masked'].join(x['context'].split(x['claim'], 1))\n",
    "        x['context_pred'] = self._predict(x['masked_context'])\n",
    "        x['context_hypothesis'] = x['masked_context'].replace(self.mask_token, x['context_pred'])\n",
    "        x['filled'] = x['masked'].replace(self.mask_token, x['context_pred'])\n",
    "        return x\n",
    "        \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['context'], dataset['context_hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt2_dir = '/run/media/jmack/ColdStorage/models/gpt2'  # Replace me with your finetuned directory\n",
    "if not os.path.exists(gpt2_dir):\n",
    "    gpt2_dir = 'gpt2'\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_dir)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_dir)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "bart_dir = '/run/media/jmack/ColdStorage/models/bart_large'  # Replace me with your finetuned directory\n",
    "if not os.path.exists(bart_dir):\n",
    "    bart_dir = \"facebook/bart-large\"\n",
    "\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example with Finetuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Thomas Jefferson founded the University of Virginia. He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874. He was a member of'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Thomas Jefferson founded the University of Virginia.\"\n",
    "masked_text = \"Thomas Jefferson founded the University of <mask>.\"\n",
    "context = text_generator(\"Thomas Jefferson founded the University of Virginia.\", max_length=75, do_sample=False)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thomas Jefferson founded the University of <mask>. He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = f\"{'.'.join(context[0]['generated_text'].split('.')[:-1])}.\"\n",
    "masked_context = context.replace(text, masked_text)\n",
    "masked_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(tokenizer, model, masked_claim: str) -> str:\n",
    "    \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "    tokens = tokenizer(masked_claim, return_tensors='pt')\n",
    "    masked_index = torch.nonzero(tokens['input_ids'][0] == tokenizer.mask_token_id, as_tuple=False)\n",
    "    # Fill mask pipeline supports only one ${mask_token} per sample\n",
    "    num = np.prod(masked_index.shape)\n",
    "    if num > 1 or num < 1:\n",
    "        print(masked_claim, tokens, masked_index)\n",
    "        raise ValueError(f\"Pipeline only supports one masked index: {num} is not supported\")\n",
    "\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits[0, masked_index.item(), :]\n",
    "    probs = logits.softmax(dim=0)\n",
    "    values, predictions = probs.topk(1)\n",
    "    word = tokenizer.decode(predictions)\n",
    "    return word.strip()\n",
    "\n",
    "predict(bart_tokenizer, bart_model, masked_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_freeze_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n",
    "bart_freeze_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PipelineWithContext(bart_freeze_tokenizer, bart_freeze_model, text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bart_cf_y_true, bart_cf_y_pred = pipe(fill=True, save=True, load_path='./data/bart_freeze_context_paper_test.csv') # Process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-15a33fdc75fec536\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-15a33fdc75fec536/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:55<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_cf_y_true, bart_cf_y_pred = pipe(load=True, path='./data/bart_freeze_context_paper_test.csv') # Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.3952908 , 0.21140143, 0.33548983]),\n",
       "  array([0.82495471, 0.09522111, 0.08362129]),\n",
       "  array([0.5344777 , 0.13130071, 0.13387424]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.31406068581022106, 0.33459903608639413, 0.26655088397924537, None),\n",
       " 0.3696047054753049)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_freeze_context_metrics, bart_freeze_context_macro, bart_freeze_context_acc = pipe.evaluate(bart_cf_y_true, bart_cf_y_pred)\n",
    "bart_freeze_context_metrics, bart_freeze_context_macro, bart_freeze_context_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Fine Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_dir = '/run/media/jmack/ColdStorage/models/gpt2'  # Replace me with your finetuned directory\n",
    "if not os.path.exists(gpt2_dir):\n",
    "    gpt2_dir = 'gpt2'\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_dir)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_dir)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_dir = '/run/media/jmack/ColdStorage/models/bart_large'\n",
    "if not os.path.exists(bart_dir):\n",
    "    bart_dir = \"facebook/bart-large\"\n",
    "    \n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PipelineWithContext(bart_tokenizer, bart_model, text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bart_cft_y_true, bart_cft_y_pred = pipe(data, mask=True, save=True, path='./data/bart_context_paper_test.csv') # Process\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0d9d57f64d77ff\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-2e0d9d57f64d77ff/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:55<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_cft_y_true, bart_cft_y_pred = pipe(load=True, path='./data/bart_context_paper_test.csv') # Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39535666, 0.22286541, 0.36896279]),\n",
       "  array([0.7982337 , 0.10984308, 0.10734854]),\n",
       "  array([0.52880288, 0.14715719, 0.16630978]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.3290616205470414, 0.3384751047226647, 0.28075661655098644, None),\n",
       " 0.3718536458783842)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_context_metrics, bart_context_macro, bart_context_acc = pipe.evaluate(bart_cft_y_true, bart_cft_y_pred)\n",
    "bart_context_metrics, bart_context_macro, bart_context_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer Pipeline\n",
    "\n",
    "- The first pipeline uses a \"Closed Book Approach\" proposed by Google.\n",
    "- The second pipeline uses an \"Open Book Approach\" using the finetuned GPT2 generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QAPipeline:\n",
    "    \n",
    "    def __init__(self, question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                 entailment_model, mask_token: str = '[MASK]'):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.mask_token = mask_token\n",
    "        self.model = entailment_model\n",
    "        self.question_tokenizer = question_tokenizer\n",
    "        self.answer_tokenizer = answer_tokenizer\n",
    "        self.vocab = answer_tokenizer.get_vocab()\n",
    "        self.max_len = min(question_tokenizer.model_max_length, answer_tokenizer.model_max_length)\n",
    "        self.stop_words = dict(zip(self.nlp.Defaults.stop_words, range(len(self.nlp.Defaults.stop_words))))\n",
    "        \n",
    "        try:\n",
    "            self.question_generator = question_generator.to(self.device)\n",
    "        except:\n",
    "            self.question_generator = question_generator\n",
    "        \n",
    "        try:\n",
    "            self.answer_generator = answer_generator.to(self.device)\n",
    "        except:\n",
    "            self.answer_generator = answer_generator\n",
    "            \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, questionize: bool = False,\n",
    "                 answer: bool = False, save: bool = False, load: bool = False, path: str = None, \n",
    "                 load_path: str = None) -> Tuple[Dataset, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            load_path = load_path if load_path else path\n",
    "            dataset = self.load(load_path)\n",
    "        \n",
    "        if dataset and not load:\n",
    "            mask = questionize = answer = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims and retrieving answers\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['answer'] != None)\n",
    "        \n",
    "        if questionize:\n",
    "            print(\"Generating questions from the claims\")\n",
    "            dataset = self.generate_questions(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    " \n",
    "        if answer:\n",
    "            print(\"Answering the questions\")\n",
    "            dataset = self.generate_answers(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return dataset, self.evaluate_dataset(dataset)\n",
    "    \n",
    "    def load(self, path: str) -> Dataset:\n",
    "        \"\"\" Loads the dataset from a file \"\"\"\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            return load_dataset('csv', data_files=[path])['train']\n",
    "        raise ValueError(f\"Path {path} does not exist or is not a file\")\n",
    "    \n",
    "    def _mask(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the last named entity in the string \"\"\"\n",
    "        x['answer'] = None\n",
    "        x['masked'] = None\n",
    "        x['label'] = self._map(x['label'])\n",
    "        claim = x['claim']\n",
    "        doc = self.nlp(claim, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        ents = doc.ents\n",
    "        if ents:\n",
    "            answer = self._get_target(ents)\n",
    "            if answer:\n",
    "                masked = self.mask_token.join(claim.rsplit(answer, 1))\n",
    "                x['answer'] = answer\n",
    "                x['masked'] = masked\n",
    "        return x\n",
    "    \n",
    "    def _get_target(self, ents) -> Optional[str]:\n",
    "        \"\"\" Gets the ideal target \"\"\"\n",
    "        target = ents[-1].text\n",
    "        if not self.stop_words.get(target.lower()):\n",
    "            return target\n",
    "        return None\n",
    "        \n",
    "    def generate_questions(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._questionize)\n",
    "        return data\n",
    "    \n",
    "    def _questionize(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the claim and fills the mask \"\"\"\n",
    "        input_text = f\"answer: {x['answer']} context: {x['claim']} </s>\"\n",
    "        features = self.question_tokenizer([input_text], return_tensors='pt').to(self.device)\n",
    "        output = self.question_generator.generate(input_ids=features['input_ids'], \n",
    "                                                  attention_mask=features['attention_mask'],\n",
    "                                                  max_length=self.max_len)\n",
    "\n",
    "        question = self.question_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        x['question'] = question.replace('question: ', '')\n",
    "        return x\n",
    "    \n",
    "    def generate_answers(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._answer_question)\n",
    "        return data\n",
    "   \n",
    "    def _answer_question(self, x: dict) -> str:\n",
    "        \"\"\" Answers the question and fills the masked claim \"\"\"\n",
    "        input_ids = self.answer_tokenizer(x['question'], return_tensors=\"pt\").input_ids\n",
    "        pred_answer = self.answer_generator.generate(input_ids.to(self.device))[0]\n",
    "        response = self.answer_tokenizer.decode(pred_answer, skip_special_tokens=True)\n",
    "        x['pred'] = response\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, response)\n",
    "        return x\n",
    "    \n",
    "    def predict_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            iters = 1\n",
    "        else:\n",
    "            iters = int(total_size / batch_size) + 1\n",
    "            \n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            if end >= total_size:\n",
    "                end = total_size\n",
    "            batch_json = self.model.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['label_probs'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['claim'], dataset['hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def evaluate(self, y_true: List[int], y_pred: List[int]) -> tuple:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        metrics = precision_recall_fscore_support(y_true, y_pred)\n",
    "        macro = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        return metrics, macro, acc\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "question_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "answer_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example with QA Fact Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = QAPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = dict(claim=\"Thomas Jefferson founded the University of Virginia.\", label='SUPPORTS')\n",
    "step1 = pipe._mask(ex)\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].',\n",
       " 'question': 'What was founded by Thomas Jefferson?'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = pipe._questionize(step1)\n",
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].',\n",
       " 'question': 'What was founded by Thomas Jefferson?',\n",
       " 'pred': 'The National Union of U.S. Banks',\n",
       " 'hypothesis': 'Thomas Jefferson founded The National Union of U.S. Banks.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = pipe._answer_question(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6609, 0.2353, 0.1037]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pipe.predict_batch([step3['claim']], [step3['hypothesis']])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = torch.argmax(pred, dim=1)\n",
    "pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = QAPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "processed, y = pipe(data, save=True, path='./data/qa_pipeline_processed.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9d72ce7d52a64b1c\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-9d72ce7d52a64b1c/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 306/306 [01:19<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(load=True, path='./data/qa_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39102715, 0.2244898 , 0.35182724]),\n",
       "  array([0.55682582, 0.23237311, 0.18710247]),\n",
       "  array([0.45942572, 0.22836344, 0.24429066]),\n",
       "  array([5948, 3645, 5660])),\n",
       " (0.3224480643689342, 0.3254338037197167, 0.31069327185750983, None),\n",
       " 0.3420966367272012)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_y_true, qa_y_pred = y\n",
    "qa_metrics, qa_macro, qa_acc = pipe.evaluate(qa_y_true, qa_y_pred)\n",
    "qa_metrics, qa_macro, qa_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pocahontas was the daughter of the president.',\n",
       "  'Pocahontas',\n",
       "  'Who was the daughter of the president?',\n",
       "  'Melania Trump was the daughter of the president.',\n",
       "  1,\n",
       "  1),\n",
       " ('There is a software that is branded with a number for the versions it is released as called Adobe Photoshop.',\n",
       "  'Adobe Photoshop',\n",
       "  'What is the software that is branded with a number for the versions it is released as?',\n",
       "  'There is a software that is branded with a number for the versions it is released as called Microsoft.',\n",
       "  0,\n",
       "  0),\n",
       " ('Chris Mullin played with a team who play basketball professionally.',\n",
       "  'Chris Mullin',\n",
       "  'Who played for a team that plays basketball professionally?',\n",
       "  'LeBron James played with a team who play basketball professionally.',\n",
       "  0,\n",
       "  1),\n",
       " ('Chris Mullin played with a team who play basketball professionally.',\n",
       "  'Chris Mullin',\n",
       "  'Who played for a team that plays basketball professionally?',\n",
       "  'LeBron James played with a team who play basketball professionally.',\n",
       "  0,\n",
       "  1),\n",
       " ('Chris Mullin played with a team who play basketball professionally.',\n",
       "  'Chris Mullin',\n",
       "  'Who played for a team that plays basketball professionally?',\n",
       "  'LeBron James played with a team who play basketball professionally.',\n",
       "  0,\n",
       "  1),\n",
       " ('Louis Malle produced films in the English language.',\n",
       "  'English',\n",
       "  'What language did Malle produce his films in?',\n",
       "  'Louis Malle produced films in the English language.',\n",
       "  0,\n",
       "  0),\n",
       " ('Color of Night won a Golden Raspberry Award in 1994.',\n",
       "  '1994',\n",
       "  'In what year did Color of Night win a Golden Raspberry Award?',\n",
       "  'Color of Night won a Golden Raspberry Award in Raspberry Pi.',\n",
       "  0,\n",
       "  0),\n",
       " ('Color of Night won a Golden Raspberry Award in 1994.',\n",
       "  '1994',\n",
       "  'In what year did Color of Night win a Golden Raspberry Award?',\n",
       "  'Color of Night won a Golden Raspberry Award in Raspberry Pi.',\n",
       "  0,\n",
       "  0),\n",
       " ('Hammer Film Productions produced Let Me In in the United States in 2010.',\n",
       "  '2010',\n",
       "  'When was Let Me In released?',\n",
       "  'Hammer Film Productions produced Let Me In in the United States in November 1973.',\n",
       "  1,\n",
       "  1),\n",
       " ('Zoe Saldana is a citizen of somewhere.',\n",
       "  'Zoe Saldana',\n",
       "  'Who is a citizen of somewhere?',\n",
       "  'It is a real life is a citizen of somewhere.',\n",
       "  1,\n",
       "  0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(processed['claim'], processed['answer'], processed['question'], processed['hypothesis'], qa_y_true, qa_y_pred))[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Book QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_utils import logger\n",
    "import logging\n",
    "\n",
    "class OpenBookPipeline(QAPipeline):\n",
    "    \n",
    "    def __init__(self, question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                 context_generator, entailment_model, mask_token: str = '[MASK]'):\n",
    "        super().__init__(question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                         entailment_model, mask_token)\n",
    "        self.text_generator = context_generator\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, questionize: bool = False,\n",
    "                 contextualize: bool = False, answer: bool = False, save: bool = False, load: bool = False, \n",
    "                 path: str = None, load_path: str = None) -> Tuple[Dataset, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            load_path = load_path if load_path else path\n",
    "            dataset = self.load(load_path)\n",
    "        \n",
    "        if dataset and not load:\n",
    "            mask = contextualize = questionize = answer = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims and retrieving answers\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['answer'] != None)\n",
    "        \n",
    "        if contextualize:\n",
    "            print(\"Generating context for claims\")\n",
    "            dataset = self.add_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        \n",
    "        if questionize:\n",
    "            print(\"Generating questions from the claims\")\n",
    "            dataset = self.generate_questions(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    " \n",
    "        if answer:\n",
    "            print(\"Answering the questions\")\n",
    "            dataset = self.generate_answers(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return dataset, self.evaluate_dataset(dataset)\n",
    "    \n",
    "    def add_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Add context to each example in the dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._contextualize)\n",
    "        return data\n",
    "    \n",
    "    def _contextualize(self, x: dict) -> dict:\n",
    "        \"\"\" Generates context for the claim \"\"\"\n",
    "        context = self.text_generator(x['claim'], max_length=100, do_sample=False)[0]['generated_text']\n",
    "        context = ''.join(context.split(x['claim'], 1))\n",
    "        if len(context) > self.max_len:\n",
    "            context = context[:self.max_len]\n",
    "        x['context'] = context.replace('\\n', ' ')  # Remove newlines to avoid error in Entailment model\n",
    "        return x\n",
    "   \n",
    "    def _answer_question(self, x: dict) -> str:\n",
    "        \"\"\" Answers the question and fills the masked claim \"\"\"\n",
    "        result = self.answer_generator(x['question'], x['context'])\n",
    "        x['pred'] = result['answer']\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, result['answer'])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "question_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "answer_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "answer_model = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_dir = '/run/media/jmack/ColdStorage/models/gpt2'  # You need to change me!\n",
    "if not os.path.exists(gpt2_dir):\n",
    "    gpt2_dir = 'gpt2'\n",
    "    \n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_dir)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_dir)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example with Open-Book Fact-Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OpenBookPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, \n",
    "                        text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = dict(claim=\"Thomas Jefferson founded the University of Virginia.\", label='SUPPORTS')\n",
    "step1 = pipe._mask(ex)\n",
    "step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].',\n",
       " 'context': ' He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874. He was a member of the Virginia State Board of Education from 1874 to 1875. He was a member of the Virginia State Board of Education from'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = pipe._contextualize(step1)\n",
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].',\n",
       " 'context': ' He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874. He was a member of the Virginia State Board of Education from 1874 to 1875. He was a member of the Virginia State Board of Education from',\n",
       " 'question': 'What was founded by Thomas Jefferson?'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = pipe._questionize(step2)\n",
    "step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claim': 'Thomas Jefferson founded the University of Virginia.',\n",
       " 'label': 0,\n",
       " 'answer': 'the University of Virginia',\n",
       " 'masked': 'Thomas Jefferson founded [MASK].',\n",
       " 'context': ' He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874. He was a member of the Virginia State Board of Education from 1874 to 1875. He was a member of the Virginia State Board of Education from',\n",
       " 'question': 'What was founded by Thomas Jefferson?',\n",
       " 'pred': 'the University of Virginia',\n",
       " 'hypothesis': 'Thomas Jefferson founded the University of Virginia.'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4 = pipe._answer_question(step3)\n",
    "step4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9629, 0.0083, 0.0289]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pipe.predict_batch([step4['claim']], [step4['hypothesis']])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = torch.argmax(pred, dim=1)\n",
    "pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OpenBookPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, \n",
    "                        text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "processed, y = pipe(data, save=True, path='./data/openbook_pipeline_processed.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5aecb15372183c51\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-5aecb15372183c51/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 306/306 [01:25<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(load=True, path='./data/openbook_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.38345771, 0.22442058, 0.34900808]),\n",
       "  array([0.51832549, 0.10891632, 0.33568905]),\n",
       "  array([0.44080641, 0.14665682, 0.34221902]),\n",
       "  array([5948, 3645, 5660])),\n",
       " (0.31896212344405517, 0.3209769524087926, 0.30989408044214223, None),\n",
       " 0.352717498197076)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_y_true, ob_y_pred = y\n",
    "openbook_metrics, openbook_macro, openbook_acc = pipe.evaluate(ob_y_true, ob_y_pred)\n",
    "openbook_metrics, openbook_macro, openbook_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What organization was the Ukrainian Soviet Socialist Republic a founding member of?',\n",
       "  'UN',\n",
       "  ' The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN Security Council.',\n",
       "  0,\n",
       "  0),\n",
       " ('What organization was the Ukrainian Soviet Socialist Republic a founding member of?',\n",
       "  'UN',\n",
       "  ' The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN Security Council.',\n",
       "  0,\n",
       "  0),\n",
       " ('What organization was the Ukrainian Soviet Socialist Republic a founding member of?',\n",
       "  'UN',\n",
       "  ' The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN Security Council.',\n",
       "  0,\n",
       "  0),\n",
       " ('How many shoulders does the New Jersey Turnpike have?',\n",
       "  'zero',\n",
       "  ' The state has a long history of bike-sharing, with the state’s first bike-sharing station in the 1930s. The state’s first bike-sharing station was built in the 1930s, and the first bike-sharing station in the state was built in the 1940s. The state’s first bike-sharing station was built in the 1940s, and the first bike-sharing station in the',\n",
       "  'The New Jersey Turnpike has 1930s shoulders.',\n",
       "  2,\n",
       "  0),\n",
       " ('What is Legendary Entertainment the owner of?',\n",
       "  'Wanda Cinemas',\n",
       "  ' The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinema.',\n",
       "  1,\n",
       "  0),\n",
       " ('What is Legendary Entertainment the owner of?',\n",
       "  'Wanda Cinemas',\n",
       "  ' The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinema.',\n",
       "  1,\n",
       "  0),\n",
       " ('What is Legendary Entertainment the owner of?',\n",
       "  'Wanda Cinemas',\n",
       "  ' The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinema.',\n",
       "  1,\n",
       "  0),\n",
       " ('What is Legendary Entertainment the owner of?',\n",
       "  'Wanda Cinemas',\n",
       "  ' The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinema.',\n",
       "  1,\n",
       "  0),\n",
       " ('What is Legendary Entertainment the owner of?',\n",
       "  'Wanda Cinemas',\n",
       "  ' The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries. Wanda is a global entertainment company that',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinema.',\n",
       "  1,\n",
       "  0),\n",
       " ('What is Aruba the only ABC Island?',\n",
       "  'ABC Island',\n",
       "  ' The show is based on the real-life story of a young woman who is kidnapped by a group of kidnappers. The show is produced by ABC Studios and is produced by Warner Bros. Television. The show is based on the real-life story of a young woman who is kidnapped by a group of kidnappers. The show is based on the real-life story of a young woman who is kidnapped by a group of kidnappers.',\n",
       "  'Aruba is the only ABC Studios.',\n",
       "  2,\n",
       "  0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(processed['question'], processed['answer'], processed['context'], processed['hypothesis'], ob_y_true, ob_y_pred))[4:14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- Reporting the precision, recall, F1 score and accuracy from all the above models\n",
    "- Short answer: it didn't work the way I expected it to. This is likely due to using models that are not fine-tuned enough on additional training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(metrics, macro, accuracy):\n",
    "    \"\"\" Prepares the results for display \"\"\"\n",
    "    macro_p, macro_r, macro_f1, _ = macro\n",
    "    precision, recall, f1_score, _ = metrics\n",
    "    metrics = np.array([precision, recall, f1_score])\n",
    "    macro = np.array([[macro_p], [macro_r], [macro_f1]])\n",
    "    results = np.hstack((metrics, macro))\n",
    "    acc = np.zeros(results.shape[1])\n",
    "    acc[-1] = accuracy\n",
    "    return np.vstack((results, acc))\n",
    "\n",
    "def report_results(options, name, results):\n",
    "    columns = [name, \"SUPPORTS\", \"NOT ENOUGH INFO\", \"REFUTES\", \"Macro\"]\n",
    "    max_len = len(max(columns, key=lambda x: len(x)))\n",
    "    header = \" | \".join('{0:{width}}'.format(col, width=max_len) for col in columns)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for i, r in enumerate(results):\n",
    "        r = np.round_(np.multiply(100, r), decimals=2)\n",
    "        line = [options[i], r[0], r[1], r[2], r[3]]\n",
    "        print(\" | \".join('{0:{width}}'.format(str(r), width=max_len) for r in line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Freeze Pipeline | SUPPORTS             | NOT ENOUGH INFO      | REFUTES              | Macro               \n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Precision            | 40.09                | 24.51                | 43.02                | 35.87               \n",
      "Recall               | 84.71                | 13.33                | 5.64                 | 34.56               \n",
      "F1 Score             | 54.42                | 17.27                | 9.97                 | 27.22               \n",
      "Accuracy             | 0.0                  | 0.0                  | 0.0                  | 38.19               \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bert_metrics, bert_macro, bert_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BERT Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Freeze Pipeline | SUPPORTS             | NOT ENOUGH INFO      | REFUTES              | Macro               \n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Precision            | 39.12                | 19.57                | 40.0                 | 32.9                \n",
      "Recall               | 91.26                | 6.21                 | 3.41                 | 33.62               \n",
      "F1 Score             | 54.76                | 9.42                 | 6.28                 | 23.49               \n",
      "Accuracy             | 0.0                  | 0.0                  | 0.0                  | 37.64               \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BART Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Freeze Pipeline | SUPPORTS                | NOT ENOUGH INFO         | REFUTES                 | Macro                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Precision               | 39.21                   | 21.29                   | 38.49                   | 33.0                   \n",
      "Recall                  | 86.23                   | 8.1                     | 6.93                    | 33.75                  \n",
      "F1 Score                | 53.9                    | 11.73                   | 11.75                   | 25.8                   \n",
      "Accuracy                | 0.0                     | 0.0                     | 0.0                     | 37.51                  \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bb_metrics, bb_macro, bb_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"RoBERTa Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBERT Freeze Pipeline | SUPPORTS               | NOT ENOUGH INFO        | REFUTES                | Macro                 \n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Precision              | 35.71                  | 20.07                  | 48.88                  | 34.89                 \n",
      "Recall                 | 85.38                  | 9.58                   | 7.29                   | 34.08                 \n",
      "F1 Score               | 50.36                  | 12.97                  | 12.69                  | 25.34                 \n",
      "Accuracy               | 0.0                    | 0.0                    | 0.0                    | 34.7                  \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(albert_metrics, albert_macro, albert_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"ALBERT Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Freeze w/Context | SUPPORTS              | NOT ENOUGH INFO       | REFUTES               | Macro                \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Precision             | 39.53                 | 21.14                 | 33.55                 | 31.41                \n",
      "Recall                | 82.5                  | 9.52                  | 8.36                  | 33.46                \n",
      "F1 Score              | 53.45                 | 13.13                 | 13.39                 | 26.66                \n",
      "Accuracy              | 0.0                   | 0.0                   | 0.0                   | 36.96                \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bart_freeze_context_metrics, bart_freeze_context_macro, bart_freeze_context_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BART Freeze w/Context\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Context Pipeline | SUPPORTS              | NOT ENOUGH INFO       | REFUTES               | Macro                \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Precision             | 39.54                 | 22.29                 | 36.9                  | 32.91                \n",
      "Recall                | 79.82                 | 10.98                 | 10.73                 | 33.85                \n",
      "F1 Score              | 52.88                 | 14.72                 | 16.63                 | 28.08                \n",
      "Accuracy              | 0.0                   | 0.0                   | 0.0                   | 37.19                \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bart_context_metrics, bart_context_macro, bart_context_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BART Context Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Pipeline     | SUPPORTS        | NOT ENOUGH INFO | REFUTES         | Macro          \n",
      "---------------------------------------------------------------------------------------\n",
      "Precision       | 39.1            | 22.45           | 35.18           | 32.24          \n",
      "Recall          | 55.68           | 23.24           | 18.71           | 32.54          \n",
      "F1 Score        | 45.94           | 22.84           | 24.43           | 31.07          \n",
      "Accuracy        | 0.0             | 0.0             | 0.0             | 34.21          \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(qa_metrics, qa_macro, qa_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"QA Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenBook Pipeline | SUPPORTS          | NOT ENOUGH INFO   | REFUTES           | Macro            \n",
      "-------------------------------------------------------------------------------------------------\n",
      "Precision         | 38.35             | 22.44             | 34.9              | 31.9             \n",
      "Recall            | 51.83             | 10.89             | 33.57             | 32.1             \n",
      "F1 Score          | 44.08             | 14.67             | 34.22             | 30.99            \n",
      "Accuracy          | 0.0               | 0.0               | 0.0               | 35.27            \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(openbook_metrics, openbook_macro, openbook_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"OpenBook Pipeline\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Examples of Frozen Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-603c3cb2a5daf17b\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-603c3cb2a5daf17b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('In The Babe, John Goodman played Babe Ruth.',\n",
       "  'In The Babe, John Goodman played Babe Ruth.',\n",
       "  0,\n",
       "  0),\n",
       " ('In The Babe, John Goodman played Babe Ruth.',\n",
       "  'In The Babe, John Goodman played Babe Ruth.',\n",
       "  0,\n",
       "  0),\n",
       " ('Islamabad is home to Quaid-i-Azam University and NUST.',\n",
       "  'Islamabad is home to Quaid-i-Azam University and NUST.',\n",
       "  1,\n",
       "  0),\n",
       " ('Simon Cowell was a part of a reality show.',\n",
       "  'Simon Cowell was a part of a reality show.',\n",
       "  0,\n",
       "  0),\n",
       " ('Simon Cowell was a part of a reality show.',\n",
       "  'Simon Cowell was a part of a reality show.',\n",
       "  0,\n",
       "  0),\n",
       " ('Simon Cowell was a part of a reality show.',\n",
       "  'Simon Cowell was a part of a reality show.',\n",
       "  0,\n",
       "  0),\n",
       " ('Simon Cowell was a part of a reality show.',\n",
       "  'Simon Cowell was a part of a reality show.',\n",
       "  0,\n",
       "  0),\n",
       " ('Simon Cowell was a part of a reality show.',\n",
       "  'Simon Cowell was a part of a reality show.',\n",
       "  0,\n",
       "  0),\n",
       " ('There are currently 15,882,417 Mormon members as of April 2017.',\n",
       "  'There are currently 15,882,417 Mormon members as of the 2017.',\n",
       "  1,\n",
       "  0),\n",
       " ('There are currently 15,882,417 Mormon members as of April 2017.',\n",
       "  'There are currently 15,882,417 Mormon members as of the 2017.',\n",
       "  1,\n",
       "  0),\n",
       " ('There are currently 15,882,417 Mormon members as of April 2017.',\n",
       "  'There are currently 15,882,417 Mormon members as of the 2017.',\n",
       "  1,\n",
       "  0),\n",
       " ('There are currently 15,882,417 Mormon members as of April 2017.',\n",
       "  'There are currently 15,882,417 Mormon members as of the 2017.',\n",
       "  1,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('Spider-Man 2 was directed by a nameless robot.',\n",
       "  'Spider-Man 2 was directed by a nameless robot.',\n",
       "  2,\n",
       "  0),\n",
       " ('The Little Prince (2015 film) is about a prince.',\n",
       "  'The Little Prince (, film) is about a prince.',\n",
       "  1,\n",
       "  0)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_data = pipe.load('./data/bart_paper_test.csv')\n",
    "list(zip(frozen_data['claim'], frozen_data['hypothesis'], bart_freeze_y_true, bart_freeze_y_pred))[1100:1120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Examples of Generated Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0d9d57f64d77ff\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-2e0d9d57f64d77ff/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [4, 5, 6, 7, 8, 9, 10, 11],\n",
       " 'claim': ['Ukrainian Soviet Socialist Republic was a founding participant of the UN.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN.',\n",
       "  'The New Jersey Turnpike has zero shoulders.',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas.'],\n",
       " 'context': ['Ukrainian Soviet Socialist Republic was a founding participant of the UN. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the UN. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'The New Jersey Turnpike has zero shoulders. The state has a long history of bike-sharing, with the state’s first bike-sharing station in the 1930s. The state’s first bike-sharing station was',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Legendary Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries'],\n",
       " 'context_hypothesis': ['Ukrainian Soviet Socialist Republic was a founding participant of the Olympics. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the Olympics. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the Olympics. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'The New Jersey Turnpike has no shoulders. The state has a long history of bike-sharing, with the state’s first bike-sharing station in the 1930s. The state’s first bike-sharing station was',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries'],\n",
       " 'context_pred': ['Olympics',\n",
       "  'Olympics',\n",
       "  'Olympics',\n",
       "  'no',\n",
       "  'Entertainment',\n",
       "  'Entertainment',\n",
       "  'Entertainment',\n",
       "  'Entertainment'],\n",
       " 'evidence_annotation_id': [344347,\n",
       "  344994,\n",
       "  344997,\n",
       "  238335,\n",
       "  178035,\n",
       "  182093,\n",
       "  314120,\n",
       "  314126],\n",
       " 'evidence_id': [327887, 328433, 328435, 240393, -1, -1, -1, -1],\n",
       " 'evidence_sentence_id': [7, 7, 7, 15, -1, -1, -1, -1],\n",
       " 'evidence_wiki_url': ['Ukrainian_Soviet_Socialist_Republic',\n",
       "  'Ukrainian_Soviet_Socialist_Republic',\n",
       "  'Ukrainian_Soviet_Socialist_Republic',\n",
       "  'New_Jersey_Turnpike',\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'filled': ['Ukrainian Soviet Socialist Republic was a founding participant of the Olympics.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the Olympics.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the Olympics.',\n",
       "  'The New Jersey Turnpike has no shoulders.',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas.',\n",
       "  'Entertainment Entertainment is the owner of Wanda Cinemas.'],\n",
       " 'id': [163803, 163803, 163803, 202314, 57085, 57085, 57085, 57085],\n",
       " 'label': [0, 0, 0, 2, 1, 1, 1, 1],\n",
       " 'masked': ['Ukrainian Soviet Socialist Republic was a founding participant of the <mask>.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the <mask>.',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the <mask>.',\n",
       "  'The New Jersey Turnpike has <mask> shoulders.',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas.',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas.',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas.',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas.'],\n",
       " 'masked_context': ['Ukrainian Soviet Socialist Republic was a founding participant of the <mask>. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the <mask>. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'Ukrainian Soviet Socialist Republic was a founding participant of the <mask>. The UN has been a major source of funding for the UN and has been instrumental in the creation of the UN Security Council. The UN has been a major source of funding',\n",
       "  'The New Jersey Turnpike has <mask> shoulders. The state has a long history of bike-sharing, with the state’s first bike-sharing station in the 1930s. The state’s first bike-sharing station was',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries',\n",
       "  '<mask> Entertainment is the owner of Wanda Cinemas. The company is a subsidiary of the company that owns the Wanda Cinema in New York City. Wanda is a global entertainment company that operates in the entertainment, entertainment, and entertainment industries'],\n",
       " 'target': ['UN',\n",
       "  'UN',\n",
       "  'UN',\n",
       "  'zero',\n",
       "  'Legendary',\n",
       "  'Legendary',\n",
       "  'Legendary',\n",
       "  'Legendary']}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_data = pipe.load('./data/bart_context_paper_test.csv')\n",
    "context_data[4:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Examples of Generated Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5aecb15372183c51\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-5aecb15372183c51/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('House of Balloons is reviewed by The Weeknd.',\n",
       "  'Weeknd',\n",
       "  'Who reviews House of Balloons?'),\n",
       " ('John Goodman did not star in 10 Cloverfield Lane.',\n",
       "  'Cloverfield Lane',\n",
       "  'What street did John Goodman not star in?'),\n",
       " (\"Chinatown's producer is a Gemini.\",\n",
       "  'Gemini',\n",
       "  \"What is Chinatown's producer?\"),\n",
       " ('Ed Decter produced the film Fargo.',\n",
       "  'Fargo',\n",
       "  'What film did Ed Decter produce?'),\n",
       " ('Ed Decter produced the film Fargo.',\n",
       "  'Fargo',\n",
       "  'What film did Ed Decter produce?'),\n",
       " ('Vatican City has yet to be established.',\n",
       "  'Vatican City',\n",
       "  'What is the name of the Vatican city that has yet to be established?'),\n",
       " ('Vatican City has yet to be established.',\n",
       "  'Vatican City',\n",
       "  'What is the name of the Vatican city that has yet to be established?'),\n",
       " ('Boxing Helena debuted in 1996.', '1996', 'When did Boxing Helena debut?'),\n",
       " ('Scaramuccia is another name for Scaramouche.',\n",
       "  'Scaramouche',\n",
       "  'What is another name for Scaramuccia?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  'British',\n",
       "  'What nationality is Sky UK?')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_data = pipe.load('./data/openbook_pipeline_processed.csv')\n",
    "list(zip(question_data['claim'], question_data['answer'], question_data['question']))[715:730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('House of Balloons is reviewed by The Weeknd.',\n",
       "  ' The Weeknd is a member of The Weeknd Family. He is a member of The Weeknd’s family. He is a member of The Weeknd’s family. He is a member of The Weeknd’s family. He is a member of The Weeknd’s family. He is a member of The Weeknd’s family. He is a member of The Weeknd’s',\n",
       "  'The Weeknd'),\n",
       " ('John Goodman did not star in 10 Cloverfield Lane.',\n",
       "  ' The film, which is based on the book of the same name by the same name, is set in the 1950s and stars John Goodman as a young man who is forced to choose between his love for his girlfriend and his love for his family. The film is set in the 1950s and stars John Goodman as a young man who is forced to choose between his love for his girlfriend and his love for his family The film is',\n",
       "  'The film is set in the 1950s'),\n",
       " (\"Chinatown's producer is a Gemini.\",\n",
       "  ' The film is directed by Michael Kors, who also stars in the film. The film is based on the novel by the same name, which is based on the novel by the same name, which is based on the novel by the same name, which is based on the novel by the same name, which is based on the novel by the same name, which is based on the novel by the same name, which is based on the novel',\n",
       "  'Michael Kors'),\n",
       " ('Ed Decter produced the film Fargo.',\n",
       "  ' The film, which is based on the novel by the same name, is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo,',\n",
       "  'Fargo'),\n",
       " ('Ed Decter produced the film Fargo.',\n",
       "  ' The film, which is based on the novel by the same name, is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo, a fictional town in the fictional South Dakota state of South Dakota. The film is set in the fictional Fargo,',\n",
       "  'Fargo'),\n",
       " ('Vatican City has yet to be established.',\n",
       "  \" The Vatican has been in the spotlight for its handling of the crisis in Venezuela, which has seen President Nicolas Maduro's government collapse and the country's oil industry collapse. The Vatican has been criticized for not doing more to help Venezuela's government, which has been accused of using its oil reserves to prop up the opposition. The Vatican has also been criticized for not doing more to help Venezuela's government, which has been accused of using its oil reserves\",\n",
       "  'Venezuela'),\n",
       " ('Vatican City has yet to be established.',\n",
       "  \" The Vatican has been in the spotlight for its handling of the crisis in Venezuela, which has seen President Nicolas Maduro's government collapse and the country's oil industry collapse. The Vatican has been criticized for not doing more to help Venezuela's government, which has been accused of using its oil reserves to prop up the opposition. The Vatican has also been criticized for not doing more to help Venezuela's government, which has been accused of using its oil reserves\",\n",
       "  'Venezuela'),\n",
       " ('Boxing Helena debuted in 1996.',\n",
       "  ' The show, which was produced by the late Tony Award-winning director of the show, was a hit with audiences and was nominated for three Tony Awards. The show was also nominated for three Tony Awards, including Best Television Performance by an Actor in a Motion Picture, Musical or Comedy, Best Television Performance by an Actress in a Motion Picture, Musical or Comedy, Best Television Performance by an Actor in a Motion Picture, Musical or Comedy, Best Television Performance by',\n",
       "  'Tony Award'),\n",
       " ('Scaramuccia is another name for Scaramouche.',\n",
       "  ' The former England captain has been linked with a move to the Premier League, but the Blues have been unable to agree a deal. The 25-year-old has been linked with a move to the Premier League, but the Blues have been unable to agree a deal The Blues have been unable to agree a deal with Scaramouche The 25-year-old has been linked with a move to the Premier League',\n",
       "  'Scaramouche'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland'),\n",
       " ('Sky UK is a British telecommunications company.',\n",
       "  ' The company operates in the UK, Ireland, the Netherlands, Sweden, Norway, Denmark, Norway, Finland, Norway, Denmark, Norway, Norway, Denmark, Norway, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway, Denmark, Norway,',\n",
       "  'Ireland')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(question_data['claim'], question_data['context'], question_data['pred']))[715:730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
