{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom Entailment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from models.textual_entailment import TextualEntailment\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "\n",
    "class Entailment:\n",
    "    \n",
    "    def __init__(self, load_elmo: bool = True, input_dim: int = 400):\n",
    "        self.input_dim = 400\n",
    "        if load_elmo:\n",
    "            self._frozen = Predictor.from_path(\"./models/textual_entailment.tar.gz\", cuda_device=0)\n",
    "        self._model = self._model()\n",
    "        \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            0: 'SUPPORTS',\n",
    "            1: 'NOT ENOUGH INFO',\n",
    "            2: 'REFUTES'\n",
    "        }\n",
    "        return switcher[label]\n",
    "\n",
    "    def _model(self) -> nn.Sequential:\n",
    "        \"\"\" Builds the MLP for the classification \"\"\"\n",
    "        model = nn.Sequential(\n",
    "          nn.Linear(400,100),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(100,3),\n",
    "          nn.Softmax(dim=1)\n",
    "        )\n",
    "        model.cuda()\n",
    "        return model\n",
    "    \n",
    "    def save(self, path: str = './models/entailment.model') -> None:\n",
    "        \"\"\" Saves the model to the file \"\"\"\n",
    "        torch.save(self._model.state_dict(), path)\n",
    "        \n",
    "    def load(self, path: str = './models/entailment.model') -> bool:\n",
    "        \"\"\" Loads the model, if possible \"\"\"\n",
    "        if os.path.exists(path):\n",
    "            self._model.load_state_dict(torch.load(path))\n",
    "            return True\n",
    "        raise ValueError(f\"Path {path} is not valid\")\n",
    "\n",
    "    def entail_dataset(self, dataset: Dataset, batch_size: int = 50, \n",
    "                       total_size: int = 30000) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        dataset = dataset.shuffle(seed=42)[:total_size]\n",
    "        X = self.entail_batch(dataset['premise'], dataset['hypothesis'], batch_size)\n",
    "        y = torch.tensor(dataset['label'])\n",
    "        return X, y\n",
    "    \n",
    "    def entail_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            \n",
    "        iters = int(total_size / batch_size)\n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            batch_json = self._frozen.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['aggregate_input'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "        \n",
    "    def predict(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> torch.Tensor:\n",
    "        \"\"\" Predicts the next word for the text \"\"\"\n",
    "        embedding = self.entail_batch(premises, hypotheses)\n",
    "        embedding = embedding.cuda()\n",
    "        preds = self._model(embedding)\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\" Evaluates the model on the test set \"\"\"\n",
    "        n, _ = X.size()\n",
    "        val = self._model(X)\n",
    "        preds = torch.argmax(val, dim=1)\n",
    "        total = float(sum(preds == y)) / n\n",
    "        return total\n",
    "\n",
    "    def fit(self, X_train: torch.Tensor, y_train: torch.Tensor, X_val: torch.Tensor, y_val: torch.Tensor, \n",
    "            batch_size: int = 32, epochs: int = 100, shuffle: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\" Fits the data on the model \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self._model.parameters(), lr=1E-3)\n",
    "        \n",
    "        X_train = X_train.cuda()\n",
    "        y_train = y_train.long().cuda()\n",
    "        X_val = X_val.cuda()\n",
    "        y_val = y_val.long().cuda()\n",
    "        \n",
    "        n, _ = X_train.shape\n",
    "        tr_iters = int(n / batch_size)\n",
    "        \n",
    "        n, _ = X_val.shape\n",
    "        val_iters = int(n / batch_size)\n",
    "        \n",
    "        train_loss = np.zeros(epochs)\n",
    "        train_acc = np.zeros(epochs)\n",
    "        validation_loss = np.zeros(epochs)\n",
    "        validation_acc = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            # Train and Validation Loss\n",
    "            total_loss = self._fit(X_train, y_train, criterion, optimizer, tr_iters, batch_size, shuffle)\n",
    "            val_loss = self._loss(X_val, y_val, criterion, val_iters, batch_size, shuffle)\n",
    "            \n",
    "            total_acc = self.evaluate(X_train, y_train)\n",
    "            val_acc = self.evaluate(X_val, y_val)\n",
    "            \n",
    "            print('[%d] loss: %.7f acc: %.7f \\t val_loss: %.7f val_acc: %.7f' % \n",
    "                  (epoch + 1, total_loss, total_acc, val_loss, val_acc)\n",
    "                 )\n",
    "            \n",
    "            train_loss[epoch] = total_loss\n",
    "            train_acc[epoch] = total_acc\n",
    "            \n",
    "            validation_loss[epoch] = val_loss\n",
    "            validation_acc[epoch] = val_acc\n",
    "        return train_loss, train_acc, validation_loss, validation_acc\n",
    "    \n",
    "    def _fit(self, X: torch.Tensor, y: torch.Tensor, criterion: nn.CrossEntropyLoss, \n",
    "             optimizer: optim.Adam, num_iters: int, batch_size: int = 32, \n",
    "             shuffle: bool = True) -> Tuple[float, float]:\n",
    "        \"\"\" Runs an epoch of training \"\"\"\n",
    "        total_loss = 0.0\n",
    "        if shuffle: \n",
    "            indices = np.random.permutation(range(X.shape[0]))\n",
    "        else:\n",
    "            indices = list(range(X.shape[0]))\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_iters):\n",
    "            end = start + batch_size\n",
    "            i = indices[start:end]\n",
    "            xi, yi = X[i], y[i]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self._model(xi)\n",
    "            loss = criterion(outputs, yi)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            start = end\n",
    "\n",
    "        total_loss /= X.shape[0]\n",
    "        return total_loss\n",
    "    \n",
    "    def _loss(self, X: torch.Tensor, y: torch.Tensor, criterion: nn.CrossEntropyLoss, \n",
    "              num_iters: int, batch_size: int = 32, shuffle: bool = True) -> Tuple[float, float]:\n",
    "        \"\"\" Runs an epoch of training \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        if shuffle: \n",
    "            indices = np.random.permutation(range(X.shape[0]))\n",
    "        else:\n",
    "            indices = list(range(X.shape[0]))\n",
    "\n",
    "        start = 0\n",
    "        for i in range(num_iters):\n",
    "            end = start + batch_size\n",
    "            i = indices[start:end]\n",
    "            xi, yi = X[i], y[i]\n",
    "\n",
    "            # forward + loss\n",
    "            outputs = self._model(xi)\n",
    "            loss = criterion(outputs, yi)\n",
    "            total_loss += loss.item()\n",
    "            start = end\n",
    "        total_loss /= X.shape[0]\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Do not run the below code unless you want to regenerate the entailments. **THIS TAKES A LONG TIME TO DO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "snli_data = load_dataset('snli')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "snli_data['train'].shape, snli_data['validation'].shape, snli_data['test'].shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "entailment = Entailment()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def get_balanced_data(entailment: Entailment, data: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Gets the training data for the entailment model \"\"\"\n",
    "    entailments = data.filter(lambda x: x['label'] == 0)\n",
    "    neutrals = data.filter(lambda x: x['label'] == 1)\n",
    "    contradictions = data.filter(lambda x: x['label'] == 2)\n",
    "    X_entail, y_entail = entailment.entail_dataset(entailments, batch_size=100, total_size=100000)\n",
    "    X_neutral, y_neutral = entailment.entail_dataset(neutrals, batch_size=100, total_size=100000)\n",
    "    X_contra, y_contra = entailment.entail_dataset(contradictions, batch_size=100, total_size=100000)\n",
    "    X_train = torch.vstack((X_entail, X_neutral))\n",
    "    X_train = torch.vstack((X_train, X_contra))\n",
    "    y_train = torch.hstack((y_entail, y_neutral))\n",
    "    y_train = torch.hstack((y_train, y_contra))\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train, y_train = get_balanced_data(entailment, snli_data['train'])\n",
    "X_train.shape, y_train.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sum(y_train == 0), sum(y_train == 1), sum(y_train == 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x_train_path = './data/x_train.pt'\n",
    "torch.save(X_train, x_train_path)\n",
    "\n",
    "y_train_path = './data/y_train.pt'\n",
    "torch.save(y_train, y_train_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "total_val = snli_data['validation'].shape[0]\n",
    "X_val, y_val = entailment.entail_dataset(snli_data['validation'], batch_size=100, total_size=total_val)\n",
    "X_val.shape, y_val.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x_val_path = './data/x_validation.pt'\n",
    "torch.save(X_val, x_val_path)\n",
    "\n",
    "y_val_path = './data/y_validation.pt'\n",
    "torch.save(y_val, y_val_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "total_test = snli_data['test'].shape[0]\n",
    "X_test, y_test = entailment.entail_dataset(snli_data['test'], batch_size=100, total_size=total_test)\n",
    "X_test.shape, y_test.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "x_test_path = './data/x_test.pt'\n",
    "torch.save(X_test, x_test_path)\n",
    "\n",
    "y_test_path = './data/y_test.pt'\n",
    "torch.save(y_test, y_test_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrays(x_path: str, y_path: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = torch.load(x_path)\n",
    "    y = torch.load(y_path)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300000, 400]), torch.Size([300000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_path = './data/x_train.pt'\n",
    "y_train_path = './data/y_train.pt'\n",
    "\n",
    "X_train, y_train = load_arrays(x_train_path, y_train_path)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(100000), tensor(100000), tensor(100000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 0), sum(y_train == 1), sum(y_train == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9842, 400]), torch.Size([9842]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_path = './data/x_validation.pt'\n",
    "y_val_path = './data/y_validation.pt'\n",
    "\n",
    "X_val, y_val = load_arrays(x_val_path, y_val_path)\n",
    "\n",
    "pos = torch.nonzero(y_val != -1)  # Remove -1\n",
    "pos = pos.reshape(pos.size()[0])  # Remove -1\n",
    "\n",
    "X_val = X_val[pos]\n",
    "y_val = y_val[pos]\n",
    "\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3329), tensor(3235))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_val == 0), sum(y_val == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9824, 400]), torch.Size([9824]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_path = './data/x_test.pt'\n",
    "y_test_path = './data/y_test.pt'\n",
    "\n",
    "X_test, y_test = load_arrays(x_test_path, y_test_path)\n",
    "\n",
    "pos = torch.nonzero(y_test != -1)  # Remove -1\n",
    "pos = pos.reshape(pos.size()[0])  # Remove -1\n",
    "\n",
    "X_test = X_test[pos]\n",
    "y_test = y_test[pos]\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3368), tensor(3219))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test == 0), sum(y_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment = Entailment(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.0205580 acc: 0.8926800 \t val_loss: 0.0215004 val_acc: 0.8585653\n",
      "[2] loss: 0.0203489 acc: 0.9008433 \t val_loss: 0.0213094 val_acc: 0.8663889\n",
      "[3] loss: 0.0202893 acc: 0.9038533 \t val_loss: 0.0212813 val_acc: 0.8653729\n",
      "[4] loss: 0.0202557 acc: 0.9017500 \t val_loss: 0.0213416 val_acc: 0.8642552\n",
      "[5] loss: 0.0202150 acc: 0.9056867 \t val_loss: 0.0212463 val_acc: 0.8672018\n",
      "[6] loss: 0.0201876 acc: 0.9050367 \t val_loss: 0.0213716 val_acc: 0.8627312\n",
      "[7] loss: 0.0201700 acc: 0.9064367 \t val_loss: 0.0212367 val_acc: 0.8686243\n",
      "[8] loss: 0.0201386 acc: 0.9074167 \t val_loss: 0.0212640 val_acc: 0.8672018\n",
      "[9] loss: 0.0201345 acc: 0.9068500 \t val_loss: 0.0212183 val_acc: 0.8687259\n",
      "[10] loss: 0.0201165 acc: 0.9089167 \t val_loss: 0.0213008 val_acc: 0.8651697\n"
     ]
    }
   ],
   "source": [
    "loss, acc, val_loss, val_acc = entailment.fit(X_train, y_train, X_val, y_val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = list(range(10))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560667752442996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailment.evaluate(X_test.cuda(), y_test.long().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560667752442996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Entailment(False)\n",
    "model.load()\n",
    "model.evaluate(X_test.cuda(), y_test.long().cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the FEVER Dataset & Entailment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset fever (/home/jmack/.cache/huggingface/datasets/fever/v1.0/1.0.0/fe391c4f48669454ae0d368997430e6fa476aacb476d930d3328b67356e74625)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18567, 7)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('fever', 'v1.0')\n",
    "data = dataset['paper_test']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: You can use the above model or this model, the results are very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmack/anaconda3/envs/final-project/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "entailment_model = Predictor.from_path(\"./models/decomposable-attention-elmo-2020.04.09.tar.gz\", cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Facebook Experiment\n",
    "\n",
    "1. Implement automatic masking\n",
    "2. Get the top 1 prediction from the LM \n",
    "3. Fill in the mask\n",
    "4. Use the claim and filled in sentence and input into an entailment model\n",
    "5. Input entailment into MLP for final fact-verification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Iterable, List, Tuple\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, tokenizer, unmasker, model):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unmasker = unmasker.to(self.device)\n",
    "        self.model = model\n",
    "        self.vocab = tokenizer.get_vocab()\n",
    "        \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, save: bool = False, load: bool = False,\n",
    "                 path: str = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            dataset = self.load(path)\n",
    "            return self.evaluate_dataset(dataset)\n",
    "        \n",
    "        print(\"Masking the claims and filling the masks for the dataset\")\n",
    "        data = self.mask_and_fill(dataset, limit)\n",
    "        if save:\n",
    "            print(\"Saving checkpoint...\")\n",
    "            data.to_csv(path)\n",
    "        return self.evaluate_dataset(data)\n",
    "    \n",
    "    def load(self, path: str) -> Dataset:\n",
    "        \"\"\" Loads the dataset from a file \"\"\"\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            return load_dataset('csv', data_files=[path])['train']\n",
    "        raise ValueError(f\"Path {path} does not exist or is not a file\")\n",
    "        \n",
    "    def mask_and_fill(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._mask)\n",
    "        data = data.filter(lambda x: x['masked'] is not None)\n",
    "        data = data.map(self._fill)\n",
    "        return data\n",
    "    \n",
    "    def _mask(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the last named entity in the string \"\"\"\n",
    "        x['target'] = None\n",
    "        x['masked'] = None\n",
    "        x['label'] = self._map(x['label'])\n",
    "        claim = x['claim']\n",
    "        doc = self.nlp(claim, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        ents = doc.ents\n",
    "        if ents:\n",
    "            target = self._get_target(ents)\n",
    "            if target:  # If the target is not in the vocab, skip the entry\n",
    "                masked = self.mask_token.join(claim.rsplit(target, 1))\n",
    "                x['target'] = target\n",
    "                x['masked'] = masked\n",
    "        return x\n",
    "    \n",
    "    def _get_target(self, ents) -> Optional[str]:\n",
    "        \"\"\" Gets the ideal target \"\"\"\n",
    "        for i in reversed(range(len(ents))):\n",
    "            words = ents[i].text.split()\n",
    "            for word in words:\n",
    "                if self.vocab.get(word):\n",
    "                    return word\n",
    "        return None\n",
    "            \n",
    "    def _fill(self, x: dict) -> dict:\n",
    "        \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "        x['pred'] = self._predict(x['masked'])\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, x['pred'])\n",
    "        return x\n",
    "    \n",
    "    def _predict(self, masked_claim: str) -> str:\n",
    "        \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "        tokens = self.tokenizer(masked_claim, return_tensors='pt')\n",
    "        masked_index = torch.nonzero(tokens['input_ids'][0] == self.tokenizer.mask_token_id, as_tuple=False)\n",
    "        # Fill mask pipeline supports only one ${mask_token} per sample\n",
    "        num = np.prod(masked_index.shape)\n",
    "        if num > 1 or num < 1:\n",
    "            print(masked_claim, tokens, masked_index)\n",
    "            raise ValueError(f\"Pipeline only supports one masked index: {num} is not supported\")\n",
    "\n",
    "        outputs = self.unmasker(**tokens.to(self.device))\n",
    "        logits = outputs.logits[0, masked_index.item(), :]\n",
    "        probs = logits.softmax(dim=0)\n",
    "        values, predictions = probs.topk(1)\n",
    "        word = self.tokenizer.decode(predictions)\n",
    "        return word.strip()\n",
    "    \n",
    "    def predict_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            iters = 1\n",
    "        else:\n",
    "            iters = int(total_size / batch_size) + 1\n",
    "            \n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            if end >= total_size:\n",
    "                end = total_size\n",
    "            batch_json = self.model.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['label_probs'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['claim'], dataset['hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def evaluate(self, y_true: List[int], y_pred: List[int]) -> tuple:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        metrics = precision_recall_fscore_support(y_true, y_pred)\n",
    "        macro = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        return metrics, macro, acc\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForMaskedLM\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-cased-whole-word-masking')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-large-cased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bert_tokenizer, bert_model, entailment_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Only needs to be run once, otherwise you can load the data directly using the second command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the claims and filling the masks for the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70915adba30e424d89e9b1f50fa99f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8fb73e0aa2463a80e5c5ceae66d26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427f11685bd1409fa9ce667bcf3daa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65336fd4455e428b857eb7fb1c426a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a0056c37ca43a5851e3b1ca61b2f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [00:44<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_y_true, bert_y_pred = pipe(data, save=True, path='./data/bert_paper_test.csv') # Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-025ce9815ef0181d\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-025ce9815ef0181d/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 285/285 [01:12<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_y_true, bert_y_pred = pipe(data, load=True, path='./data/bert_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.40040702, 0.24450704, 0.46094946]),\n",
       "  array([0.85714286, 0.12667834, 0.05694287]),\n",
       "  array([0.54583285, 0.16689098, 0.10136387]),\n",
       "  array([5509, 3426, 5286])),\n",
       " (0.36862117579333087, 0.3469213557286139, 0.27136256672250425, None),\n",
       " 0.3837282891498488)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_metrics, bert_macro, bert_acc = pipe.evaluate(bert_y_true, bert_y_pred)\n",
    "bert_metrics, bert_macro, bert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broad Check on Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART\n",
    "\n",
    "- Unknown training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "bart_tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bart_tokenizer, bart_model, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the claims and filling the masks for the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0922939a6b7a4c7183ffade7f4c53caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b13b8ac6afe47f2818e994e31fb1867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3682fe02dcf64fdcb549b9e4216b816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11561.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c31605c11a44ffd991b7a4f65402b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554c90cade5447839783542583e2b09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:42<00:00,  3.88it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_y_true, bart_y_pred = pipe(data, save=True, path='./data/bart_paper_test.csv') # Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-603c3cb2a5daf17b\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-603c3cb2a5daf17b/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:00<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_y_true, bart_y_pred = pipe(data, load=True, path='./data/bart_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39114821, 0.19481982, 0.4       ]),\n",
       "  array([0.91259058, 0.06169757, 0.03409353]),\n",
       "  array([0.54759155, 0.09371614, 0.06283167]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.3286560096930863, 0.3361272271467631, 0.23471312063635574, None),\n",
       " 0.37635152668454286)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc = pipe.evaluate(bart_y_true, bart_y_pred)\n",
    "bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "- Trained on Bookcorpus, Wikipedia and CC News datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-large were not used when initializing BigBirdForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdTokenizer, BigBirdForMaskedLM\n",
    "  \n",
    "bigbird_tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-large\")\n",
    "bigbird_model = BigBirdForMaskedLM.from_pretrained(\"google/bigbird-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(bigbird_tokenizer, bigbird_model, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the claims and filling the masks for the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32ef32fb9fb42a4945fa65ba2c088c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d79c1599b8849a0813466e37671ca93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff156b85df54c10b6db04423f52252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11561.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 14 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b542a155ccf04754848b64b4b806220c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8ffad097b546c4a80b6b594a3e8d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/212 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [00:54<00:00,  3.90it/s]\n"
     ]
    }
   ],
   "source": [
    "bigbird_y_true, bigbird_y_pred = pipe(data, save=True, path='./data/bigbird_paper_test.csv') # Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c87a382d2406e7ce\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-c87a382d2406e7ce/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:00<00:00,  3.81it/s]\n"
     ]
    }
   ],
   "source": [
    "bigbird_y_true, bigbird_y_pred = pipe(data, load=True, path='./data/bigbird_paper_test.csv') # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39213263, 0.21348315, 0.38491049]),\n",
       "  array([0.86231884, 0.08131241, 0.06933886]),\n",
       "  array([0.53910951, 0.1177686 , 0.11750927]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.330175421699136, 0.337656704478242, 0.25812912452361464, None),\n",
       " 0.3751405587751925)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_metrics, bb_macro, bb_acc = pipe.evaluate(bigbird_y_true, bigbird_y_pred)\n",
    "bb_metrics, bb_macro, bb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT v2\n",
    "\n",
    "- Model was trained on Bookcorpus and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast, AlbertForMaskedLM\n",
    "  \n",
    "albert_tokenizer = AlbertTokenizerFast.from_pretrained(\"albert-large-v2\")\n",
    "albert_model = AlbertForMaskedLM.from_pretrained(\"albert-large-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(albert_tokenizer, albert_model, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_y_true, albert_y_pred = pipe(data, save=True, path='./data/albert_paper_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e7b121b58b205f1\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-8e7b121b58b205f1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 102/102 [00:27<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "albert_y_true, albert_y_pred = pipe(data, load=True, path='./data/albert_paper_test.csv')  # Load & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.3572282 , 0.20067454, 0.49044586]),\n",
       "  array([0.85379783, 0.0958132 , 0.07336827]),\n",
       "  array([0.5037062 , 0.12970027, 0.12764194]),\n",
       "  array([1751, 1242, 2099])),\n",
       " (0.34944953068893625, 0.34099310164181434, 0.2536828038116286, None),\n",
       " 0.3472113118617439)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_metrics, albert_macro, albert_acc = pipe.evaluate(albert_y_true, albert_y_pred)\n",
    "albert_metrics, albert_macro, albert_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting a Context Layer into the Pipeline\n",
    "\n",
    "- We will try using the autoregressive language model GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT2 on CC News Dataset\n",
    "\n",
    "1. Open up a terminal, navigate to `/path/to/project/finetuning/` and run `run_clm.sh`\n",
    "2. Wait until it finishes\n",
    "3. **Note**: This only trains GPT2 on 150,000 samples of the CC News dataset. Please adjust the script to suit both your GPU settings and your fine-tuning needs\n",
    "4. I plan to upload the model fine-tuned on the complete dataset at a later date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning BART on FEVER\n",
    "\n",
    "1. Open up a terminal, navigate to `/path/to/project/finetuning/` and run `run_mlm.sh`\n",
    "2. Wait until it finishes\n",
    "3. **Note**: This only trains BART on 150,000 samples of the FEVER dataset. Please adjust the script to suit both your GPU settings and your fine-tuning needs\n",
    "4. I plan to upload the model fine-tuned on the complete dataset at a later date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the Pipeline - Adding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_utils import logger\n",
    "import logging\n",
    "\n",
    "\n",
    "class PipelineWithContext(Pipeline):\n",
    "    \n",
    "    def __init__(self, tokenizer, unmasker, text_generator, model):\n",
    "        super().__init__(tokenizer, unmasker, model)\n",
    "        self.text_generator = text_generator\n",
    "        self.max_len = tokenizer.model_max_length\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, context: bool = False,\n",
    "                 fill: bool = False, save: bool = False, load: bool = False, \n",
    "                 path: str = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            dataset = self.load(path)\n",
    "        \n",
    "        if dataset and not load:\n",
    "            context = True\n",
    "            fill = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims...\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['target'] != None)\n",
    "        \n",
    "        if context:\n",
    "            print(\"Adding context to the dataset\")\n",
    "            dataset = self.add_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        if fill:\n",
    "            print(\"Filling the context masks\")\n",
    "            dataset = self.fill_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return self.evaluate_dataset(dataset)\n",
    "        \n",
    "    def add_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._contextualize)\n",
    "        return data\n",
    "    \n",
    "    def _contextualize(self, x: dict) -> str:\n",
    "        \"\"\" Generates context for the claim \"\"\"\n",
    "        context = self.text_generator(x['claim'], max_length=50, do_sample=False)[0]['generated_text']\n",
    "        if len(context) > self.max_len:\n",
    "            context = context[:self.max_len]\n",
    "        x['context'] = context.replace('\\n', ' ')  # Remove newlines to avoid error in Entailment model\n",
    "        return x\n",
    "    \n",
    "    def fill_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Fills the context map \"\"\"\n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._fill)\n",
    "        return data\n",
    "    \n",
    "    def _fill(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the claim and fills the mask \"\"\"\n",
    "        x['masked_context'] = x['masked'].join(x['context'].split(x['claim'], 1))\n",
    "        x['context_pred'] = self._predict(x['masked_context'])\n",
    "        x['context_hypothesis'] = x['masked_context'].replace(self.mask_token, x['context_pred'])\n",
    "        x['filled'] = x['masked'].replace(self.mask_token, x['context_pred'])\n",
    "        return x\n",
    "        \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['context'], dataset['context_hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import pipeline\n",
    "\n",
    "gpt2_dir = '/run/media/jmack/ColdStorage/models/gpt2'\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_dir)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_dir)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "bart_dir = '/run/media/jmack/ColdStorage/models/bart_large'\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example with Finetuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Thomas Jefferson founded the University of Virginia. He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874. He was a member of'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Thomas Jefferson founded the University of Virginia.\"\n",
    "masked_text = \"Thomas Jefferson founded the University of <mask>.\"\n",
    "context = text_generator(\"Thomas Jefferson founded the University of Virginia.\", max_length=75, do_sample=False)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thomas Jefferson founded the University of <mask>. He was born in Richmond, Virginia, on July 4, 1847. He graduated from the University of Virginia in 1871. He was a member of the Virginia Board of Trustees from 1871 to 1873. He was a member of the Virginia State Board of Education from 1873 to 1874.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = f\"{'.'.join(context[0]['generated_text'].split('.')[:-1])}.\"\n",
    "masked_context = context.replace(text, masked_text)\n",
    "masked_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(tokenizer, model, masked_claim: str) -> str:\n",
    "    \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "    tokens = tokenizer(masked_claim, return_tensors='pt')\n",
    "    masked_index = torch.nonzero(tokens['input_ids'][0] == tokenizer.mask_token_id, as_tuple=False)\n",
    "    # Fill mask pipeline supports only one ${mask_token} per sample\n",
    "    num = np.prod(masked_index.shape)\n",
    "    if num > 1 or num < 1:\n",
    "        print(masked_claim, tokens, masked_index)\n",
    "        raise ValueError(f\"Pipeline only supports one masked index: {num} is not supported\")\n",
    "\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits[0, masked_index.item(), :]\n",
    "    probs = logits.softmax(dim=0)\n",
    "    values, predictions = probs.topk(1)\n",
    "    word = tokenizer.decode(predictions)\n",
    "    return word.strip()\n",
    "\n",
    "predict(bart_tokenizer, bart_model, masked_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = PipelineWithContext(bart_tokenizer, bart_model, text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_y_true, bart_y_pred = pipe(data, mask=True, save=True, path='./data/bart_context_paper_test.csv') # Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0d9d57f64d77ff\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-2e0d9d57f64d77ff/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 232/232 [01:54<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "bart_y_true, bart_y_pred = pipe(data, load=True, path='./data/bart_context_paper_test.csv') # Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.39515043, 0.22238267, 0.36750789]),\n",
       "  array([0.79710145, 0.10984308, 0.10734854]),\n",
       "  array([0.52836986, 0.1470518 , 0.16616153]),\n",
       "  array([4416, 2804, 4341])),\n",
       " (0.3283469948327742, 0.3380976892637275, 0.28052772911980967, None),\n",
       " 0.3714211573393305)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_context_metrics, bart_context_macro, bart_context_acc = pipe.evaluate(bart_y_true, bart_y_pred)\n",
    "bart_context_metrics, bart_context_macro, bart_context_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer Pipeline\n",
    "\n",
    "- The first pipeline uses a \"Closed Book Approach\" proposed by Google.\n",
    "- The second pipeline uses an \"Open Book Approach\" using the finetuned GPT2 generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QAPipeline:\n",
    "    \n",
    "    def __init__(self, question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                 entailment_model, mask_token: str = '[MASK]'):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.mask_token = mask_token\n",
    "        self.model = entailment_model\n",
    "        self.question_tokenizer = question_tokenizer\n",
    "        self.answer_tokenizer = answer_tokenizer\n",
    "        self.vocab = answer_tokenizer.get_vocab()\n",
    "        self.max_len = min(question_tokenizer.model_max_length, answer_tokenizer.model_max_length)\n",
    "        \n",
    "        try:\n",
    "            self.question_generator = question_generator.to(self.device)\n",
    "        except:\n",
    "            self.question_generator = question_generator\n",
    "        \n",
    "        try:\n",
    "            self.answer_generator = answer_generator.to(self.device)\n",
    "        except:\n",
    "            self.answer_generator = answer_generator\n",
    "            \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, questionize: bool = False,\n",
    "                 answer: bool = False, save: bool = False, load: bool = False, \n",
    "                 path: str = None) -> Tuple[Dataset, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load:\n",
    "            dataset = self.load(path)\n",
    "        \n",
    "        if dataset and not load:\n",
    "            mask = questionize = answer = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims and retrieving answers\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['answer'] != None)\n",
    "        \n",
    "        if questionize:\n",
    "            print(\"Generating questions from the claims\")\n",
    "            dataset = self.generate_questions(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    " \n",
    "        if answer:\n",
    "            print(\"Answering the questions\")\n",
    "            dataset = self.generate_answers(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return dataset, self.evaluate_dataset(dataset)\n",
    "    \n",
    "    def load(self, path: str) -> Dataset:\n",
    "        \"\"\" Loads the dataset from a file \"\"\"\n",
    "        if os.path.exists(path) and os.path.isfile(path):\n",
    "            return load_dataset('csv', data_files=[path])['train']\n",
    "        raise ValueError(f\"Path {path} does not exist or is not a file\")\n",
    "    \n",
    "    def _mask(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the last named entity in the string \"\"\"\n",
    "        x['answer'] = None\n",
    "        x['masked'] = None\n",
    "        x['label'] = self._map(x['label'])\n",
    "        claim = x['claim']\n",
    "        doc = self.nlp(claim, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "        ents = doc.ents\n",
    "        if ents:\n",
    "            answer = self._get_target(ents)\n",
    "            masked = self.mask_token.join(claim.rsplit(answer, 1))\n",
    "            x['answer'] = answer\n",
    "            x['masked'] = masked\n",
    "        return x\n",
    "    \n",
    "    def _get_target(self, ents) -> Optional[str]:\n",
    "        \"\"\" Gets the ideal target \"\"\"\n",
    "        for i in reversed(range(len(ents))):\n",
    "            words = ents[i].text.split()\n",
    "            for word in words:\n",
    "                if self.vocab.get(word):\n",
    "                    return word\n",
    "        return None\n",
    "        \n",
    "    def generate_questions(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._questionize)\n",
    "        return data\n",
    "    \n",
    "    def _questionize(self, x: dict) -> dict:\n",
    "        \"\"\" Masks the claim and fills the mask \"\"\"\n",
    "        input_text = f\"answer: {x['answer']} context: {x['claim']} </s>\"\n",
    "        features = self.question_tokenizer([input_text], return_tensors='pt').to(self.device)\n",
    "        output = self.question_generator.generate(input_ids=features['input_ids'], \n",
    "                                                  attention_mask=features['attention_mask'],\n",
    "                                                  max_length=self.max_len)\n",
    "\n",
    "        question = self.question_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        x['question'] = question.replace('question: ', '')\n",
    "        return x\n",
    "    \n",
    "    def generate_answers(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Masks the entire dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._answer_question)\n",
    "        return data\n",
    "   \n",
    "    def _answer_question(self, x: dict) -> str:\n",
    "        \"\"\" Answers the question and fills the masked claim \"\"\"\n",
    "        input_ids = self.answer_tokenizer(x['question'], return_tensors=\"pt\").input_ids\n",
    "        pred_answer = self.answer_generator.generate(input_ids.to(self.device))[0]\n",
    "        response = self.answer_tokenizer.decode(pred_answer, skip_special_tokens=True)\n",
    "        x['pred'] = response\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, response)\n",
    "        return x\n",
    "    \n",
    "    def predict_batch(self, premises: List[str], hypotheses: List[str], batch_size: int = 50) -> np.ndarray:\n",
    "        \"\"\" Entails all the data and returns the entailed texts \"\"\"\n",
    "        batches = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        total_size = len(batches)\n",
    "        \n",
    "        if total_size < batch_size:\n",
    "            batch_size = total_size\n",
    "            iters = 1\n",
    "        else:\n",
    "            iters = int(total_size / batch_size) + 1\n",
    "            \n",
    "        X = list()\n",
    "        \n",
    "        start = 0\n",
    "        for j in tqdm(range(iters)):\n",
    "            end = start + batch_size\n",
    "            if end >= total_size:\n",
    "                end = total_size\n",
    "            batch_json = self.model.predict_batch_json(batches[start:end])\n",
    "            X.extend([e['label_probs'] for e in batch_json])\n",
    "            start = end\n",
    "        return torch.tensor(X)\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        premise, hypothesis = dataset['claim'], dataset['hypothesis']\n",
    "        y_pred = torch.argmax(self.predict_batch(premise, hypothesis), dim=1)\n",
    "        y_true = np.array(dataset['label'])\n",
    "        return y_true, y_pred.numpy()\n",
    "    \n",
    "    def evaluate(self, y_true: List[int], y_pred: List[int]) -> tuple:\n",
    "        \"\"\" Evaluates how well the model is able to predict \"\"\"\n",
    "        metrics = precision_recall_fscore_support(y_true, y_pred)\n",
    "        macro = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        return metrics, macro, acc\n",
    "    \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            'SUPPORTS': 0,\n",
    "            'NOT ENOUGH INFO': 1,\n",
    "            'REFUTES': 2\n",
    "        }\n",
    "        return switcher[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "question_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "answer_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = QAPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the claims and retrieving answers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cf8f8241644668b95c083ed37cbfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcbad413f8849f797fca939f9f622ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating questions from the claims\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d777da334b4943b71de77787221c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7059.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n",
      "Answering the questions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007cc52177f8444eb48df24c306e5bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7059.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407877f1807340a09747e8be1d26ccda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f5ebd62b43456f8e685ac262eddc57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:39<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(data, save=True, path='./data/qa_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d892ef2f6cc43aa0\n",
      "Reusing dataset csv (/home/jmack/.cache/huggingface/datasets/csv/default-d892ef2f6cc43aa0/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 142/142 [00:38<00:00,  3.64it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(data, load=True, path='./data/qa_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.37243764, 0.21875   , 0.34508671]),\n",
       "  array([0.55625231, 0.15963512, 0.23014649]),\n",
       "  array([0.44615385, 0.18457482, 0.27613321]),\n",
       "  array([2711, 1754, 2594])),\n",
       " (0.31209144804183436, 0.315344639017696, 0.30228729162191853, None),\n",
       " 0.3378665533361666)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_pred = y\n",
    "qa_metrics, qa_macro, qa_acc = pipe.evaluate(y_true, y_pred)\n",
    "qa_metrics, qa_macro, qa_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Book QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_utils import logger\n",
    "import logging\n",
    "\n",
    "class OpenBookPipeline(QAPipeline):\n",
    "    \n",
    "    def __init__(self, question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                 context_generator, entailment_model, mask_token: str = '[MASK]'):\n",
    "        super().__init__(question_tokenizer, question_generator, answer_tokenizer, answer_generator, \n",
    "                         entailment_model, mask_token)\n",
    "        self.text_generator = context_generator\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    def __call__(self, dataset: Dataset = None, limit: int = 0, mask: bool = False, questionize: bool = False,\n",
    "                 contextualize: bool = False, answer: bool = False, save: bool = False, load: bool = False, \n",
    "                 path: str = None) -> Tuple[Dataset, Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\" Performs all operations on the dataset \"\"\"\n",
    "        if (save or load) and not path:\n",
    "            raise ValueError(\"Must pass a path to save the processed dataset\")\n",
    "        \n",
    "        if load and os.path.exists(path):\n",
    "            dataset = load_dataset('csv', data_files=[path])['train']\n",
    "        \n",
    "        if dataset and not load:\n",
    "            mask = contextualize = questionize = answer = True\n",
    "        \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        \n",
    "        if mask:\n",
    "            print(\"Masking the claims and retrieving answers\")\n",
    "            dataset = dataset.map(self._mask)\n",
    "            dataset = dataset.filter(lambda x: x['answer'] != None)\n",
    "        \n",
    "        if contextualize:\n",
    "            print(\"Generating context for claims\")\n",
    "            dataset = self.add_context(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        \n",
    "        if questionize:\n",
    "            print(\"Generating questions from the claims\")\n",
    "            dataset = self.generate_questions(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    " \n",
    "        if answer:\n",
    "            print(\"Answering the questions\")\n",
    "            dataset = self.generate_answers(dataset)\n",
    "            if save:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                dataset.to_csv(path)\n",
    "        return dataset, self.evaluate_dataset(dataset)\n",
    "    \n",
    "    def add_context(self, dataset: Dataset, limit: int = 0) -> Dataset:\n",
    "        \"\"\" Add context to each example in the dataset \"\"\"            \n",
    "        if limit != 0:\n",
    "            dataset = dataset.select(list(range(limit)))\n",
    "        data = dataset.map(self._contextualize)\n",
    "        return data\n",
    "    \n",
    "    def _contextualize(self, x: dict) -> dict:\n",
    "        \"\"\" Generates context for the claim \"\"\"\n",
    "        context = self.text_generator(x['claim'], max_length=100, do_sample=False)[0]['generated_text']\n",
    "        context = ''.join(context.split(x['claim'], 1))\n",
    "        if len(context) > self.max_len:\n",
    "            context = context[:self.max_len]\n",
    "        x['context'] = context.replace('\\n', ' ')  # Remove newlines to avoid error in Entailment model\n",
    "        return x\n",
    "   \n",
    "    def _answer_question(self, x: dict) -> str:\n",
    "        \"\"\" Answers the question and fills the masked claim \"\"\"\n",
    "        result = self.answer_generator(x['question'], x['context'])\n",
    "        x['pred'] = result['answer']\n",
    "        x['hypothesis'] = x['masked'].replace(self.mask_token, result['answer'])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "question_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "answer_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "answer_model = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_dir = '/run/media/jmack/ColdStorage/models/gpt2'\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_dir)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_dir)\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=gpt2_model, tokenizer=gpt2_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = OpenBookPipeline(question_tokenizer, question_model, answer_tokenizer, answer_model, \n",
    "                        text_generator, entailment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking the claims and retrieving answers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc49531f97f464ebc34b5b8a261ad0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f897c76bd94c97b5780c57587f759c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating context for claims\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee5fa81c5084564be53605f95fa452f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n",
      "Generating questions from the claims\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f7f6f4d58445fcbd4e4498c5152bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n",
      "Answering the questions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ac31fbf4934b5b80dedd4fd77f989d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14221.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/285 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [01:17<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(data, save=True, path='./data/openbook_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-75779cb36afa620c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jmack/.cache/huggingface/datasets/csv/default-75779cb36afa620c/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/285 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jmack/.cache/huggingface/datasets/csv/default-75779cb36afa620c/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [01:17<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "processed, y = pipe(data, load=True, path='./data/openbook_pipeline_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0.37913349, 0.20913771, 0.35089388]),\n",
       "  array([0.5098929 , 0.09486281, 0.34903519]),\n",
       "  array([0.43489704, 0.13052209, 0.34996206]),\n",
       "  array([5509, 3426, 5286])),\n",
       " (0.313055023812211, 0.3179303011957723, 0.305127064990746, None),\n",
       " 0.3501160255959497)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_pred = y\n",
    "openbook_metrics, openbook_macro, openbook_acc = pipe.evaluate(y_true, y_pred)\n",
    "openbook_metrics, openbook_macro, openbook_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14200],\n",
       "       [14201],\n",
       "       [14207],\n",
       "       [14208],\n",
       "       [14209],\n",
       "       [14210],\n",
       "       [14212],\n",
       "       [14213],\n",
       "       [14216],\n",
       "       [14220]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argwhere(y_true != y_pred)\n",
    "indices[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'answer': ['12',\n",
       "   '12',\n",
       "   'Block',\n",
       "   'Block',\n",
       "   'Block',\n",
       "   'Block',\n",
       "   'Apple',\n",
       "   'Lorraine',\n",
       "   'Donna',\n",
       "   'John'],\n",
       "  'claim': [\"Heroes' first season had 12 episodes.\",\n",
       "   \"Heroes' first season had 12 episodes.\",\n",
       "   'The Block (album) contains only an unofficial single.',\n",
       "   'The Block (album) contains only an unofficial single.',\n",
       "   'The Block (album) contains only an unofficial single.',\n",
       "   'The Block (album) contains only an unofficial single.',\n",
       "   'The IPhone 4 was designed by Apple Inc..',\n",
       "   'Ed and Lorraine Warren were married.',\n",
       "   'Donna Noble is played through improv.',\n",
       "   'John Frusciante worked at an accounting firm.'],\n",
       "  'context': [\" The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. (AP Photo/J. Scott Applewhite) The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. (AP Photo/J.\",\n",
       "   \" The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. (AP Photo/J. Scott Applewhite) The show's first season was a hit with critics and fans alike, and it was nominated for six Golden Globe Awards. (AP Photo/J.\",\n",
       "   ' The album is a collaboration between the two artists, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The',\n",
       "   ' The album is a collaboration between the two artists, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The',\n",
       "   ' The album is a collaboration between the two artists, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The',\n",
       "   ' The album is a collaboration between the two artists, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The album is produced by the duo, who have been working together since 2012. The',\n",
       "   ' The IPhone 4 is a new phone that is designed to be a better alternative to the iPhone 7 Plus. The IPhone 4 is a new phone that is designed to be a better alternative to the iPhone 7 Plus. The IPhone 4 is a new phone that is designed to be a better alternative to the iPhone 7 Plus. The IPhone 4 is a new phone that is designed to be a better alternative to the iPhone 7',\n",
       "   ' The couple had two children together, a daughter, a son and a daughter-in-law. The couple had two children together, a daughter, a son and daughter-in-law. The couple had two children together, a daughter, a son and daughter-in-law. The couple had two children together, a daughter, a son and daughter-in-law. The couple had two children together, a daughter,',\n",
       "   ' The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National Theatre of Canada. The show is produced by the National',\n",
       "   ' He was a member of the board of directors of the company. “I’m not sure what the board is going to do,” Frusciante said. “I’m not sure what the board is going to do.” The board is expected to meet again on Wednesday. The board is expected to vote on a resolution to approve the sale of the company. The company has'],\n",
       "  'evidence_annotation_id': [167374,\n",
       "   167375,\n",
       "   263457,\n",
       "   263457,\n",
       "   263464,\n",
       "   263464,\n",
       "   167036,\n",
       "   46805,\n",
       "   111196,\n",
       "   162394],\n",
       "  'evidence_id': [181222,\n",
       "   181223,\n",
       "   261126,\n",
       "   261128,\n",
       "   261138,\n",
       "   261139,\n",
       "   180881,\n",
       "   55824,\n",
       "   -1,\n",
       "   -1],\n",
       "  'evidence_sentence_id': [8, 8, 6, 7, 6, 8, -1, -1, -1, -1],\n",
       "  'evidence_wiki_url': ['Heroes_-LRB-TV_series-RRB-',\n",
       "   'Heroes_-LRB-TV_series-RRB-',\n",
       "   'The_Block_-LRB-album-RRB-',\n",
       "   'The_Block_-LRB-album-RRB-',\n",
       "   'The_Block_-LRB-album-RRB-',\n",
       "   'The_Block_-LRB-album-RRB-',\n",
       "   'IPhone_4',\n",
       "   'Ed_and_Lorraine_Warren',\n",
       "   '',\n",
       "   ''],\n",
       "  'hypothesis': [\"Heroes' first season had six episodes.\",\n",
       "   \"Heroes' first season had six episodes.\",\n",
       "   'The a collaboration between the two artists, who have been working together since 2012. (album) contains only an unofficial single.',\n",
       "   'The a collaboration between the two artists, who have been working together since 2012. (album) contains only an unofficial single.',\n",
       "   'The a collaboration between the two artists, who have been working together since 2012. (album) contains only an unofficial single.',\n",
       "   'The a collaboration between the two artists, who have been working together since 2012. (album) contains only an unofficial single.',\n",
       "   'The IPhone 4 was designed by iPhone 7 Plus Inc..',\n",
       "   'Ed and daughter, a son and a daughter-in-law Warren were married.',\n",
       "   'The show is produced by the National Theatre of Canada Noble is played through improv.',\n",
       "   'Frusciante Frusciante worked at an accounting firm.'],\n",
       "  'id': [144200,\n",
       "   144200,\n",
       "   221250,\n",
       "   221250,\n",
       "   221250,\n",
       "   221250,\n",
       "   143905,\n",
       "   30640,\n",
       "   93826,\n",
       "   139535],\n",
       "  'label': [2, 2, 2, 2, 2, 2, 0, 0, 1, 1],\n",
       "  'masked': [\"Heroes' first season had [MASK] episodes.\",\n",
       "   \"Heroes' first season had [MASK] episodes.\",\n",
       "   'The [MASK] (album) contains only an unofficial single.',\n",
       "   'The [MASK] (album) contains only an unofficial single.',\n",
       "   'The [MASK] (album) contains only an unofficial single.',\n",
       "   'The [MASK] (album) contains only an unofficial single.',\n",
       "   'The IPhone 4 was designed by [MASK] Inc..',\n",
       "   'Ed and [MASK] Warren were married.',\n",
       "   '[MASK] Noble is played through improv.',\n",
       "   '[MASK] Frusciante worked at an accounting firm.'],\n",
       "  'pred': ['six',\n",
       "   'six',\n",
       "   'a collaboration between the two artists, who have been working together since 2012.',\n",
       "   'a collaboration between the two artists, who have been working together since 2012.',\n",
       "   'a collaboration between the two artists, who have been working together since 2012.',\n",
       "   'a collaboration between the two artists, who have been working together since 2012.',\n",
       "   'iPhone 7 Plus',\n",
       "   'daughter, a son and a daughter-in-law',\n",
       "   'The show is produced by the National Theatre of Canada',\n",
       "   'Frusciante'],\n",
       "  'question': ['How many episodes did the first season of Heroes have?',\n",
       "   'How many episodes did the first season of Heroes have?',\n",
       "   'What is the name of the album that contains only an unofficial single?',\n",
       "   'What is the name of the album that contains only an unofficial single?',\n",
       "   'What is the name of the album that contains only an unofficial single?',\n",
       "   'What is the name of the album that contains only an unofficial single?',\n",
       "   'Who designed the IPhone 4?',\n",
       "   \"What was the name of Ed Warren's wife?\",\n",
       "   'What is the name of the character that is played through improv?',\n",
       "   'Who was the founder of the accounting firm?']},\n",
       " array([0, 0, 2, 2, 2, 2, 0, 2, 2, 0]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[14200:14210], y_pred[14200:14210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- Reporting the precision, recall, F1 score and accuracy from all the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(metrics, macro, accuracy):\n",
    "    \"\"\" Prepares the results for display \"\"\"\n",
    "    macro_p, macro_r, macro_f1, _ = macro\n",
    "    precision, recall, f1_score, _ = metrics\n",
    "    metrics = np.array([precision, recall, f1_score])\n",
    "    macro = np.array([[macro_p], [macro_r], [macro_f1]])\n",
    "    results = np.hstack((metrics, macro))\n",
    "    acc = np.zeros(results.shape[1])\n",
    "    acc[-1] = accuracy\n",
    "    return np.vstack((results, acc))\n",
    "\n",
    "def report_results(options, name, results):\n",
    "    columns = [name, \"SUPPORTS\", \"NOT ENOUGH INFO\", \"REFUTES\", \"Macro\"]\n",
    "    max_len = len(max(columns, key=lambda x: len(x)))\n",
    "    header = \" | \".join('{0:{width}}'.format(col, width=max_len) for col in columns)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    for i, r in enumerate(results):\n",
    "        r = np.round_(np.multiply(100, r), decimals=2)\n",
    "        line = [options[i], r[0], r[1], r[2], r[3]]\n",
    "        print(\" | \".join('{0:{width}}'.format(str(r), width=max_len) for r in line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Freeze Pipeline | SUPPORTS             | NOT ENOUGH INFO      | REFUTES              | Macro               \n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Precision            | 40.04                | 24.45                | 46.09                | 36.86               \n",
      "Recall               | 85.71                | 12.67                | 5.69                 | 34.69               \n",
      "F1 Score             | 54.58                | 16.69                | 10.14                | 27.14               \n",
      "Accuracy             | 0.0                  | 0.0                  | 0.0                  | 38.37               \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bert_metrics, bert_macro, bert_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BERT Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Freeze Pipeline | SUPPORTS             | NOT ENOUGH INFO      | REFUTES              | Macro               \n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Precision            | 39.11                | 19.48                | 40.0                 | 32.87               \n",
      "Recall               | 91.26                | 6.17                 | 3.41                 | 33.61               \n",
      "F1 Score             | 54.76                | 9.37                 | 6.28                 | 23.47               \n",
      "Accuracy             | 0.0                  | 0.0                  | 0.0                  | 37.64               \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bart_freeze_metrics, bart_freeze_macro, bart_freeze_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BART Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Freeze Pipeline | SUPPORTS                | NOT ENOUGH INFO         | REFUTES                 | Macro                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Precision               | 39.21                   | 21.35                   | 38.49                   | 33.02                  \n",
      "Recall                  | 86.23                   | 8.13                    | 6.93                    | 33.77                  \n",
      "F1 Score                | 53.91                   | 11.78                   | 11.75                   | 25.81                  \n",
      "Accuracy                | 0.0                     | 0.0                     | 0.0                     | 37.51                  \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bb_metrics, bb_macro, bb_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"RoBERTa Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBERT Freeze Pipeline | SUPPORTS               | NOT ENOUGH INFO        | REFUTES                | Macro                 \n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "Precision              | 35.72                  | 20.07                  | 49.04                  | 34.94                 \n",
      "Recall                 | 85.38                  | 9.58                   | 7.34                   | 34.1                  \n",
      "F1 Score               | 50.37                  | 12.97                  | 12.76                  | 25.37                 \n",
      "Accuracy               | 0.0                    | 0.0                    | 0.0                    | 34.72                 \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(albert_metrics, albert_macro, albert_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"ALBERT Freeze Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Context Pipeline | SUPPORTS              | NOT ENOUGH INFO       | REFUTES               | Macro                \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "Precision             | 39.52                 | 22.24                 | 36.75                 | 32.83                \n",
      "Recall                | 79.71                 | 10.98                 | 10.73                 | 33.81                \n",
      "F1 Score              | 52.84                 | 14.71                 | 16.62                 | 28.05                \n",
      "Accuracy              | 0.0                   | 0.0                   | 0.0                   | 37.14                \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(bart_context_metrics, bart_context_macro, bart_context_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"BART Context Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Pipeline     | SUPPORTS        | NOT ENOUGH INFO | REFUTES         | Macro          \n",
      "---------------------------------------------------------------------------------------\n",
      "Precision       | 37.24           | 21.88           | 34.51           | 31.21          \n",
      "Recall          | 55.63           | 15.96           | 23.01           | 31.53          \n",
      "F1 Score        | 44.62           | 18.46           | 27.61           | 30.23          \n",
      "Accuracy        | 0.0             | 0.0             | 0.0             | 33.79          \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(qa_metrics, qa_macro, qa_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"QA Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenBook Pipeline | SUPPORTS          | NOT ENOUGH INFO   | REFUTES           | Macro            \n",
      "-------------------------------------------------------------------------------------------------\n",
      "Precision         | 37.91             | 20.91             | 35.09             | 31.31            \n",
      "Recall            | 50.99             | 9.49              | 34.9              | 31.79            \n",
      "F1 Score          | 43.49             | 13.05             | 35.0              | 30.51            \n",
      "Accuracy          | 0.0               | 0.0               | 0.0               | 35.01            \n"
     ]
    }
   ],
   "source": [
    "results = prepare_results(openbook_metrics, openbook_macro, openbook_acc)\n",
    "report_results((\"Precision\", \"Recall\", \"F1 Score\", \"Accuracy\"), \"OpenBook Pipeline\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
