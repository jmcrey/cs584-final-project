{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-29 20:05:07--  https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.102.11\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.102.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1713485474 (1.6G) [application/zip]\n",
      "Saving to: ‘/tmp/wiki-pages.zip’\n",
      "\n",
      "/tmp/wiki-pages.zip 100%[===================>]   1.60G  1.16MB/s    in 4m 19s  \n",
      "\n",
      "2021-04-29 20:09:27 (6.30 MB/s) - ‘/tmp/wiki-pages.zip’ saved [1713485474/1713485474]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip -O /tmp/wiki-pages.zip\n",
    "!unzip -q /tmp/wiki-pages.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-29 20:13:53--  https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.106.10\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.106.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33024303 (31M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘./data/train.jsonl’\n",
      "\n",
      "./data/train.jsonl  100%[===================>]  31.49M  17.3MB/s    in 1.8s    \n",
      "\n",
      "2021-04-29 20:13:55 (17.3 MB/s) - ‘./data/train.jsonl’ saved [33024303/33024303]\n",
      "\n",
      "--2021-04-29 20:13:55--  https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_dev.jsonl\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.106.10\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.106.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4349935 (4.1M) [binary/octet-stream]\n",
      "Saving to: ‘./data/share_task_dev.jsonl’\n",
      "\n",
      "./data/share_task_d 100%[===================>]   4.15M  5.66MB/s    in 0.7s    \n",
      "\n",
      "2021-04-29 20:13:56 (5.66 MB/s) - ‘./data/share_task_dev.jsonl’ saved [4349935/4349935]\n",
      "\n",
      "--2021-04-29 20:13:56--  https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_test.jsonl\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.106.10\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.106.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1599159 (1.5M) [binary/octet-stream]\n",
      "Saving to: ‘./data/share_task_test.jsonl’\n",
      "\n",
      "./data/share_task_t 100%[===================>]   1.52M  2.43MB/s    in 0.6s    \n",
      "\n",
      "2021-04-29 20:13:58 (2.43 MB/s) - ‘./data/share_task_test.jsonl’ saved [1599159/1599159]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl -O ./data/train.jsonl\n",
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_dev.jsonl -O ./data/share_task_dev.jsonl\n",
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_test.jsonl -O ./data/share_task_test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom Entailment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import History\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from models.textual_entailment import TextualEntailment\n",
    "from datasets.arrow_dataset import Dataset\n",
    "\n",
    "\n",
    "class Entailment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._frozen = Predictor.from_path(\"./models/textual_entailment.tar.gz\")\n",
    "        self._model = self._model()\n",
    "        \n",
    "    def _map(self, label: int) -> str:\n",
    "        \"\"\" Maps the label to the value \"\"\"\n",
    "        switcher = {\n",
    "            0: 'entailment',\n",
    "            1: 'contradiction'\n",
    "        }\n",
    "        return switcher[label]\n",
    "        \n",
    "    def _entail_texts(self, premises: List[str], hypotheses: List[str]) -> np.ndarray:\n",
    "        \"\"\" Entails the texts \"\"\"\n",
    "        batch_json = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        entailment = self._frozen.predict_batch_json(batch_json)\n",
    "        return np.array([e['aggregate_input'] for e in entailment])\n",
    "        \n",
    "    def _model(self) -> Sequential:\n",
    "        \"\"\" Builds the MLP for the classification \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_shape=(400,)))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "        return model\n",
    "    \n",
    "    def prepare(self, data: Dataset) -> Dataset:\n",
    "        \"\"\" Prepares the data by removing neutral statements and splitting the data into data and labels \"\"\"\n",
    "        data = data.filter(lambda x: x['label'] != 1)\n",
    "        \n",
    "        def change_label(x):\n",
    "            if x['label'] == 2:\n",
    "                x['label'] = 1\n",
    "            return x\n",
    "        \n",
    "        data = data.map(change_label)\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def forward(self, premises: List[str], hypotheses: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\" Predicts the next word for the text \"\"\"\n",
    "        entailments = self._entail_texts(texts, claims)\n",
    "        preds = self.model.predict(entailments)\n",
    "        return preds\n",
    "    \n",
    "    def fit(self, train: Dataset, validation: Dataset, batch_size: int = 32, epochs: int = 200) -> History:\n",
    "        \"\"\" Fits the model on the training and validation sets \"\"\"\n",
    "        \n",
    "        def fit_generator(data: Dataset, batch_size: int = 32, epochs: int = 200):\n",
    "            n, d = data.shape\n",
    "            iters = int(n / batch_size)\n",
    "            \n",
    "            for i in range(epochs):\n",
    "                start = 0\n",
    "                for j in range(iters):\n",
    "                    end = start + batch_size\n",
    "                    batch = data[start:end]\n",
    "                    premises, hypotheses, labels = (batch['premise'], batch['hypothesis'], batch['label'])\n",
    "                    x = self._entail_texts(premises, hypotheses)\n",
    "                    y = np.array(labels)\n",
    "                    yield x, y\n",
    "                    start = end\n",
    "        \n",
    "        tr_epoch_steps = int(train.shape[0] / batch_size)\n",
    "        tr_generator = fit_generator(train, batch_size, epochs)\n",
    "        \n",
    "        val_epoch_steps = int(validation.shape[0] / batch_size)\n",
    "        val_generator = fit_generator(validation, batch_size, epochs)\n",
    "        history = self._model.fit(tr_generator, steps_per_epoch=tr_epoch_steps, epochs=epochs,\n",
    "                                  validation_data=val_generator, validation_steps=val_epoch_steps)\n",
    "        return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/jmack/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "snli_data = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment = Entailment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d289ab09e24ad8ab841419d454bca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e425de21a2411a86a05ba13ab5dc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=551.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef5ef50e58e4e8fa0e4ee93cbe97961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5457c1b474d4a4ba3bf9b62c68b387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6781.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476809c2e4c64027a129b806d1dc12d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=367388.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93d679633e240ff91ff135f09c0d3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=6765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = entailment.prepare(snli_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "   50/11480 [..............................] - ETA: 3:26:40 - loss: 0.3935 - acc: 0.8472"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-2eea9130760a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentailment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-9497f62fc954>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train, validation, batch_size, epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mval_epoch_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mval_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         history = self._model.fit(tr_generator, steps_per_epoch=tr_epoch_steps, epochs=epochs,\n\u001b[0m\u001b[1;32m     85\u001b[0m                                   validation_data=val_generator, validation_steps=val_epoch_steps)\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "entailment.fit(data['train'], data['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Facebook Experiment\n",
    "\n",
    "1. Implement automatic masking\n",
    "2. Get the top 1 prediction from the LM \n",
    "3. Fill in the mask\n",
    "4. Use the claim and filled in sentence and input into an entailment model\n",
    "5. Input entailment into MLP for final fact-verification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import Iterable, List, Tuple\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, tokenizer, unmasker, predictor, mask_token: str = \"[MASK]\"):\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\")\n",
    "        self.mask_token = mask_token\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unmasker = unmasker\n",
    "        self.predictor = predictor\n",
    "        self.model = self._model()\n",
    "    \n",
    "    def _mask(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\" Masks the last named entity in the string \"\"\"\n",
    "        masked_sents = list()\n",
    "        for doc in self.nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "            ents = doc.ents\n",
    "            target = ents[-1].text.split()[-1]\n",
    "            masked = doc.text.replace(target, mask_token)\n",
    "            masked_sents.append(masked)\n",
    "        return masked_sents\n",
    "    \n",
    "    def _fill_mask(self, texts: Iterable) -> List[str]:\n",
    "        \"\"\" Fills the masked token with the  top-1 predicted value \"\"\"\n",
    "        preds = list()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenizer(text, return_tensors='tf')['input_ids']\n",
    "            masked_index = tf.where(tokens[0] == self.tokenizer.mask_token_id).numpy()\n",
    "\n",
    "            outputs = self.unmasker(tokens)\n",
    "            logits = outputs.logits[0, masked_index.item(), :]\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            topk = tf.math.top_k(probs, k=1)\n",
    "            values, predictions = topk.values.numpy(), topk.indices.numpy()\n",
    "            \n",
    "            pred = tokenizer.decode(predictions)\n",
    "            preds.append(text.replace(self.mask_token, pred))\n",
    "        return preds\n",
    "    \n",
    "    def _entail_texts(self, premises: List[str], hypotheses: List[str]) -> np.ndarray:\n",
    "        \"\"\" Entails the texts \"\"\"\n",
    "        batch_json = [dict(premise=v[0], hypothesis=v[1]) for v in zip(premises, hypotheses)]\n",
    "        print(batch_json)\n",
    "        entailment = self.predictor.predict_batch_json(batch_json)\n",
    "        return np.array([e['aggregate_input'] for e in entailment])\n",
    "        \n",
    "    \n",
    "    def _model(self) -> Sequential:\n",
    "        \"\"\" Builds the MLP for the classification \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', input_shape=(400,)))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                      metrics=['acc'])\n",
    "        return model\n",
    "    \n",
    "    def forward(self, texts: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\" Predicts the next word for the text \"\"\"\n",
    "        masked = self._mask(texts)\n",
    "        claims = self._fill_mask(masked)\n",
    "        entailments = self._entail_texts(texts, claims)\n",
    "        preds = self.model.predict(entailments)\n",
    "        return preds\n",
    "    \n",
    "    def fit()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from models.textual_entailment import TextualEntailment\n",
    "\n",
    "predictor = Predictor.from_path(\"./models/textual_entailment.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "unmasker = TFBertForMaskedLM.from_pretrained('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"Thomas Jefferson founded the University of Virginia after retiring.\"\n",
    "test2 = \"Microsoft's headquarters are in Redmond.\"\n",
    "test3 = \"Tim Roth is an English actor.\"\n",
    "texts = [test1,test2,test3]\n",
    "mask_token = \"[MASK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'premise': 'Thomas Jefferson founded the University of Virginia after retiring.', 'hypothesis': 'Thomas Jefferson founded the University of Virginia after retiring.'}, {'premise': \"Microsoft's headquarters are in Redmond.\", 'hypothesis': \"Microsoft's headquarters are in Atlanta.\"}, {'premise': 'Tim Roth is an English actor.', 'hypothesis': 'Tim Roth is an American actor.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6196344],\n",
       "       [0.5279083],\n",
       "       [0.5934663]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(tokenizer, unmasker, predictor)\n",
    "pipe.forward(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2550"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['Virginia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import Iterable\n",
    "\n",
    "NLP = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def mask_last_named_entity(texts: Iterable, mask_token: str = \"[MASK]\") -> str:\n",
    "    \"\"\" Masks the last named entity in the string \"\"\"\n",
    "    masked_sents = list()\n",
    "    for doc in NLP.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "        ents = doc.ents\n",
    "        target = ents[-1].text.split()[-1]\n",
    "        masked = doc.text.replace(target, mask_token)\n",
    "        masked_sents.append(masked)\n",
    "    return masked_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas Jefferson founded the University of [MASK] after retiring.',\n",
       " \"Microsoft's headquarters are in [MASK].\",\n",
       " 'Tim Roth is an [MASK] actor.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = mask_last_named_entity([test1, test2, test3], mask_token) \n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokenizer, model, sentences: list) -> list:\n",
    "    filled = list()\n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer(sent, return_tensors='tf')['input_ids']\n",
    "        masked_index = tf.where(tokens[0] == tokenizer.mask_token_id).numpy()\n",
    "        \n",
    "        outputs = model(tokens)\n",
    "        logits = outputs.logits[0, masked_index.item(), :]\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        topk = tf.math.top_k(probs, k=1)\n",
    "        values, predictions = topk.values.numpy(), topk.indices.numpy()\n",
    "        \n",
    "        pred = tokenizer.decode(predictions)\n",
    "        filled.append(sent.replace('[MASK]', pred))\n",
    "    return filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas Jefferson founded the University of Virginia after retiring.',\n",
       " \"Microsoft's headquarters are in Atlanta.\",\n",
       " 'Tim Roth is an American actor.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(tokenizer, model, masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor.predict(\n",
    "        premise=\"Two women are wandering along the shore drinking iced tea.\",\n",
    "        hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = np.array([outputs['aggregate_input']])\n",
    "model_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 40,201\n",
      "Trainable params: 40,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(400,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.64905727]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lerc is not a registered model.\n",
      "roberta-rte is not a registered model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'textual_entailment'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp_models import pretrained\n",
    "\n",
    "pretrained.get_pretrained_models()['pair-classification-decomposable-attention-elmo'].task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 6 required positional arguments: 'vocab', 'text_field_embedder', 'attend_feedforward', 'matrix_attention', 'compare_feedforward', and 'aggregate_feedforward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cd6fa6b91752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpair_classification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecomposableAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecomposableAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 6 required positional arguments: 'vocab', 'text_field_embedder', 'attend_feedforward', 'matrix_attention', 'compare_feedforward', and 'aggregate_feedforward'"
     ]
    }
   ],
   "source": [
    "from allennlp_models.pair_classification.models import DecomposableAttention\n",
    "\n",
    "model = DecomposableAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lerc is not a registered model.\n",
      "roberta-rte is not a registered model.\n"
     ]
    }
   ],
   "source": [
    "pred = pretrained.load_predictor('pair-classification-decomposable-attention-elmo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pred.predict(\n",
    "        premise=\"Two women are wandering along the shore drinking iced tea.\",\n",
    "        hypothesis=\"Two women are sitting on a blanket near some rocks talking about politics.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_logits': [-3.349437952041626, 4.429553985595703, 0.9683454036712646],\n",
       " 'label_probs': [0.00040553370490670204,\n",
       "  0.9691703915596008,\n",
       "  0.030424002557992935],\n",
       " 'h2p_attention': [[0.6542813777923584,\n",
       "   0.04181642830371857,\n",
       "   0.044751670211553574,\n",
       "   0.04774125665426254,\n",
       "   0.032621681690216064,\n",
       "   0.02951720543205738,\n",
       "   0.0222022645175457,\n",
       "   0.02251223288476467,\n",
       "   0.022536294534802437,\n",
       "   0.03395495191216469,\n",
       "   0.025392329320311546,\n",
       "   0.02267230674624443],\n",
       "  [2.8718852263409644e-05,\n",
       "   0.9997602105140686,\n",
       "   2.671044239832554e-05,\n",
       "   3.0569222872145474e-05,\n",
       "   1.630610495340079e-05,\n",
       "   1.6656915249768645e-05,\n",
       "   2.2438669475377537e-05,\n",
       "   1.677172986092046e-05,\n",
       "   1.8310134692001157e-05,\n",
       "   3.356780143803917e-05,\n",
       "   1.4835497495369054e-05,\n",
       "   1.4929611097613815e-05],\n",
       "  [0.10177356749773026,\n",
       "   0.08759545534849167,\n",
       "   0.1118234395980835,\n",
       "   0.11648961156606674,\n",
       "   0.08897500485181808,\n",
       "   0.10866256058216095,\n",
       "   0.06278194487094879,\n",
       "   0.06688541173934937,\n",
       "   0.06301344931125641,\n",
       "   0.07898420095443726,\n",
       "   0.06009376421570778,\n",
       "   0.05292144790291786],\n",
       "  [0.0004623697604984045,\n",
       "   0.0005366479163058102,\n",
       "   0.0013094748137518764,\n",
       "   0.9844369888305664,\n",
       "   0.0037109607364982367,\n",
       "   0.0008001072565093637,\n",
       "   0.000759141577873379,\n",
       "   0.00399616826325655,\n",
       "   0.0005880572716705501,\n",
       "   0.0022712009958922863,\n",
       "   0.0006370970513671637,\n",
       "   0.0004917914629913867],\n",
       "  [0.021879466250538826,\n",
       "   0.016263877972960472,\n",
       "   0.043620962649583817,\n",
       "   0.3232453167438507,\n",
       "   0.12519335746765137,\n",
       "   0.21580788493156433,\n",
       "   0.03572555258870125,\n",
       "   0.09783807396888733,\n",
       "   0.03638046607375145,\n",
       "   0.04659603163599968,\n",
       "   0.022638076916337013,\n",
       "   0.014810925349593163],\n",
       "  [0.017246725037693977,\n",
       "   0.011929051950573921,\n",
       "   0.031438130885362625,\n",
       "   0.2378472089767456,\n",
       "   0.06237583979964256,\n",
       "   0.4272131025791168,\n",
       "   0.038840629160404205,\n",
       "   0.05050673708319664,\n",
       "   0.05539732053875923,\n",
       "   0.04054877907037735,\n",
       "   0.016226788982748985,\n",
       "   0.010429643094539642],\n",
       "  [0.0102377999573946,\n",
       "   0.010660983622074127,\n",
       "   0.012148184701800346,\n",
       "   0.4098280966281891,\n",
       "   0.024485282599925995,\n",
       "   0.049786731600761414,\n",
       "   0.36476778984069824,\n",
       "   0.028135929256677628,\n",
       "   0.031522396951913834,\n",
       "   0.043300505727529526,\n",
       "   0.008045729249715805,\n",
       "   0.007080606184899807],\n",
       "  [0.012575305067002773,\n",
       "   0.013238711282610893,\n",
       "   0.032401192933321,\n",
       "   0.24183233082294464,\n",
       "   0.13379737734794617,\n",
       "   0.3817344009876251,\n",
       "   0.06835024803876877,\n",
       "   0.05440527945756912,\n",
       "   0.015210701152682304,\n",
       "   0.02560754492878914,\n",
       "   0.012246701866388321,\n",
       "   0.008600177243351936],\n",
       "  [0.02550632879137993,\n",
       "   0.02194177731871605,\n",
       "   0.04952565208077431,\n",
       "   0.0795864388346672,\n",
       "   0.0690709576010704,\n",
       "   0.5571142435073853,\n",
       "   0.05692204087972641,\n",
       "   0.03753482177853584,\n",
       "   0.030629552900791168,\n",
       "   0.04115668684244156,\n",
       "   0.017573071643710136,\n",
       "   0.013438398018479347],\n",
       "  [0.0032999140676110983,\n",
       "   0.0037106797099113464,\n",
       "   0.00489591620862484,\n",
       "   0.01287841983139515,\n",
       "   0.009935688227415085,\n",
       "   0.050064265727996826,\n",
       "   0.6458783745765686,\n",
       "   0.012672769837081432,\n",
       "   0.16958104074001312,\n",
       "   0.08052573353052139,\n",
       "   0.0033406356815248728,\n",
       "   0.0032165131997317076],\n",
       "  [0.011913090944290161,\n",
       "   0.009290369227528572,\n",
       "   0.0199927669018507,\n",
       "   0.42288175225257874,\n",
       "   0.02823151834309101,\n",
       "   0.02082984149456024,\n",
       "   0.014663374982774258,\n",
       "   0.33708497881889343,\n",
       "   0.058575790375471115,\n",
       "   0.05190961807966232,\n",
       "   0.017143506556749344,\n",
       "   0.007483404595404863],\n",
       "  [0.028186608105897903,\n",
       "   0.025777099654078484,\n",
       "   0.05010043457150459,\n",
       "   0.40535101294517517,\n",
       "   0.07974492758512497,\n",
       "   0.07758960872888565,\n",
       "   0.0279350895434618,\n",
       "   0.1694972813129425,\n",
       "   0.04901993274688721,\n",
       "   0.05365008860826492,\n",
       "   0.021943798288702965,\n",
       "   0.011204180307686329],\n",
       "  [0.003791020717471838,\n",
       "   0.0037876616697758436,\n",
       "   0.005148179829120636,\n",
       "   0.04601581394672394,\n",
       "   0.007641100324690342,\n",
       "   0.009489400312304497,\n",
       "   0.01279785018414259,\n",
       "   0.11390943080186844,\n",
       "   0.20984220504760742,\n",
       "   0.5805724263191223,\n",
       "   0.0038965558633208275,\n",
       "   0.0031084204092621803],\n",
       "  [0.07601422071456909,\n",
       "   0.06725233048200607,\n",
       "   0.08519984036684036,\n",
       "   0.13195280730724335,\n",
       "   0.07711151987314224,\n",
       "   0.08093569427728653,\n",
       "   0.0706259161233902,\n",
       "   0.09599754959344864,\n",
       "   0.08104491233825684,\n",
       "   0.07409657537937164,\n",
       "   0.09086937457323074,\n",
       "   0.06889919191598892],\n",
       "  [0.08347291499376297,\n",
       "   0.08220735937356949,\n",
       "   0.08219696581363678,\n",
       "   0.08517542481422424,\n",
       "   0.07966078072786331,\n",
       "   0.08381462097167969,\n",
       "   0.0845530554652214,\n",
       "   0.08125219494104385,\n",
       "   0.08679300546646118,\n",
       "   0.0875125303864479,\n",
       "   0.08170206844806671,\n",
       "   0.08165907859802246]],\n",
       " 'p2h_attention': [[0.5794978737831116,\n",
       "   0.038077544420957565,\n",
       "   0.037115372717380524,\n",
       "   0.01881016418337822,\n",
       "   0.02877109684050083,\n",
       "   0.03250500187277794,\n",
       "   0.0274263434112072,\n",
       "   0.028178894892334938,\n",
       "   0.03750938922166824,\n",
       "   0.02097991108894348,\n",
       "   0.03461119532585144,\n",
       "   0.050881993025541306,\n",
       "   0.024034641683101654,\n",
       "   0.02161012962460518,\n",
       "   0.01999048702418804],\n",
       "  [2.7932510420214385e-05,\n",
       "   0.9997096061706543,\n",
       "   2.4092194507829845e-05,\n",
       "   1.646525925025344e-05,\n",
       "   1.6129462892422453e-05,\n",
       "   1.6956086255959235e-05,\n",
       "   2.1539437511819415e-05,\n",
       "   2.2373153115040623e-05,\n",
       "   2.4335489797522314e-05,\n",
       "   1.7792224753065966e-05,\n",
       "   2.0356405002530664e-05,\n",
       "   3.509387170197442e-05,\n",
       "   1.8110436940332875e-05,\n",
       "   1.4419360013562255e-05,\n",
       "   1.4847881175228395e-05],\n",
       "  [0.055058859288692474,\n",
       "   0.04919420927762985,\n",
       "   0.05664772167801857,\n",
       "   0.07399989664554596,\n",
       "   0.07967934757471085,\n",
       "   0.082305908203125,\n",
       "   0.04520675912499428,\n",
       "   0.10085493326187134,\n",
       "   0.10117033123970032,\n",
       "   0.043238017708063126,\n",
       "   0.0806855633854866,\n",
       "   0.12563005089759827,\n",
       "   0.04533838480710983,\n",
       "   0.03364589437842369,\n",
       "   0.027344146743416786],\n",
       "  [0.0009355745278298855,\n",
       "   0.000896776095032692,\n",
       "   0.0009399468544870615,\n",
       "   0.8861116766929626,\n",
       "   0.009404795244336128,\n",
       "   0.009918340481817722,\n",
       "   0.024291837587952614,\n",
       "   0.011989947408437729,\n",
       "   0.002589575247839093,\n",
       "   0.0018115945858880877,\n",
       "   0.027183692902326584,\n",
       "   0.01619011163711548,\n",
       "   0.006454849150031805,\n",
       "   0.0008300007320940495,\n",
       "   0.0004513250896707177],\n",
       "  [0.021218551322817802,\n",
       "   0.015877211466431618,\n",
       "   0.023829173296689987,\n",
       "   0.11086936295032501,\n",
       "   0.12089911103248596,\n",
       "   0.08633403480052948,\n",
       "   0.04817131906747818,\n",
       "   0.22017863392829895,\n",
       "   0.07459499686956406,\n",
       "   0.04638965427875519,\n",
       "   0.060234926640987396,\n",
       "   0.10571739077568054,\n",
       "   0.035576194524765015,\n",
       "   0.01609918661415577,\n",
       "   0.014010204002261162],\n",
       "  [0.007183174602687359,\n",
       "   0.006068067625164986,\n",
       "   0.01088811457157135,\n",
       "   0.008943452499806881,\n",
       "   0.0779723972082138,\n",
       "   0.2212289720773697,\n",
       "   0.03664618358016014,\n",
       "   0.23502855002880096,\n",
       "   0.22510765492916107,\n",
       "   0.08745463192462921,\n",
       "   0.016627691686153412,\n",
       "   0.0384838730096817,\n",
       "   0.016530055552721024,\n",
       "   0.0063220299780368805,\n",
       "   0.005515076220035553],\n",
       "  [0.0034150348510593176,\n",
       "   0.005166654475033283,\n",
       "   0.00397616159170866,\n",
       "   0.0053633530624210835,\n",
       "   0.008158475160598755,\n",
       "   0.01271277666091919,\n",
       "   0.16970250010490417,\n",
       "   0.026598431169986725,\n",
       "   0.01453727763146162,\n",
       "   0.713119387626648,\n",
       "   0.007398379035294056,\n",
       "   0.008757534436881542,\n",
       "   0.014090587384998798,\n",
       "   0.0034868817310780287,\n",
       "   0.003516556229442358],\n",
       "  [0.007020140998065472,\n",
       "   0.00782924797385931,\n",
       "   0.008587962947785854,\n",
       "   0.05723832547664642,\n",
       "   0.045296810567379,\n",
       "   0.03351450338959694,\n",
       "   0.02653764933347702,\n",
       "   0.042922645807266235,\n",
       "   0.019434189423918724,\n",
       "   0.028366943821310997,\n",
       "   0.3448033630847931,\n",
       "   0.10772685706615448,\n",
       "   0.25426188111305237,\n",
       "   0.009608656167984009,\n",
       "   0.006850983947515488],\n",
       "  [0.006401711143553257,\n",
       "   0.007786095142364502,\n",
       "   0.007370183244347572,\n",
       "   0.007672714535146952,\n",
       "   0.01534313801676035,\n",
       "   0.03348563611507416,\n",
       "   0.0270836241543293,\n",
       "   0.0109315300360322,\n",
       "   0.014446379616856575,\n",
       "   0.3457837700843811,\n",
       "   0.05458037927746773,\n",
       "   0.02838052064180374,\n",
       "   0.4266784191131592,\n",
       "   0.007389492355287075,\n",
       "   0.006666360888630152],\n",
       "  [0.005955484230071306,\n",
       "   0.00881356280297041,\n",
       "   0.005704079754650593,\n",
       "   0.01829722709953785,\n",
       "   0.01213375199586153,\n",
       "   0.015133792534470558,\n",
       "   0.022971050813794136,\n",
       "   0.011363179422914982,\n",
       "   0.011985580436885357,\n",
       "   0.10138233751058578,\n",
       "   0.029865272343158722,\n",
       "   0.019178668037056923,\n",
       "   0.7288942933082581,\n",
       "   0.004171451088041067,\n",
       "   0.004150253254920244],\n",
       "  [0.055401433259248734,\n",
       "   0.0484546460211277,\n",
       "   0.05398578941822052,\n",
       "   0.06384691596031189,\n",
       "   0.07333149760961533,\n",
       "   0.07533685117959976,\n",
       "   0.05309552699327469,\n",
       "   0.06760141998529434,\n",
       "   0.0636606439948082,\n",
       "   0.052319228649139404,\n",
       "   0.12269391119480133,\n",
       "   0.09758078306913376,\n",
       "   0.06085463985800743,\n",
       "   0.06363722681999207,\n",
       "   0.04819945618510246],\n",
       "  [0.06747952848672867,\n",
       "   0.06651806831359863,\n",
       "   0.06485441327095032,\n",
       "   0.06723154336214066,\n",
       "   0.06544717401266098,\n",
       "   0.06605444103479385,\n",
       "   0.0637412816286087,\n",
       "   0.0647592544555664,\n",
       "   0.06640926003456116,\n",
       "   0.06871878355741501,\n",
       "   0.07306013256311417,\n",
       "   0.0679658055305481,\n",
       "   0.06622321903705597,\n",
       "   0.06582117825746536,\n",
       "   0.06571603566408157]],\n",
       " 'premise_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'wandering',\n",
       "  'along',\n",
       "  'the',\n",
       "  'shore',\n",
       "  'drinking',\n",
       "  'iced',\n",
       "  'tea',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'hypothesis_tokens': ['Two',\n",
       "  'women',\n",
       "  'are',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'a',\n",
       "  'blanket',\n",
       "  'near',\n",
       "  'some',\n",
       "  'rocks',\n",
       "  'talking',\n",
       "  'about',\n",
       "  'politics',\n",
       "  '.',\n",
       "  '@@NULL@@'],\n",
       " 'label': 'contradiction'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMA Probe into BART\n",
    "\n",
    "The goal of this section is to understand the amount of knowledge stored in BART using the [LAMA probe](https://github.com/facebookresearch/LAMA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From BERT to BART\n",
    "\n",
    "Insert the BART model in lieu of BERT to see if performance increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting a Context Layer into the Pipeline\n",
    "\n",
    "- We will try two methods, one using DrQA and one using an autoregressive language model GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
